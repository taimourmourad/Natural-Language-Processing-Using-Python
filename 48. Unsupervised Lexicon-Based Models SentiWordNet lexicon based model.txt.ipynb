{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd11f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "import utils as ut\n",
    "import nltk\n",
    "import textblob\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b103db",
   "metadata": {},
   "source": [
    "## load our IMDB review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c467474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie_reviews.csv.bz2', compression='bz2')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba01373",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d70764",
   "metadata": {},
   "source": [
    "for word 'awesome`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade93a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Polarity Score: 0.875\n",
      "Negative Polarity Score: 0.125\n",
      "Objective Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score:', awesome.pos_score())\n",
    "print('Negative Polarity Score:', awesome.neg_score())\n",
    "print('Objective Score:', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14ac45",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f332e1",
   "metadata": {},
   "source": [
    "Letâ€™s now build a generic function to extract and aggregate sentiment scores for a\n",
    "complete textual document based on matched synsets in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b86876ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
    "                                         norm_neg_score, norm_final_score]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                             ['Predicted Sentiment', 'Objectivity',\n",
    "                                                              'Positive', 'Negative', 'Overall']], \n",
    "                                                             codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc189d8f",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4156e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            negative        0.71     0.09      0.2   -0.11\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.74      0.2     0.06    0.14\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive         0.8     0.14     0.07    0.07\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd313b",
   "metadata": {},
   "source": [
    "# Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190b555",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  codes=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                codes=level_labels)) \n",
    "    print(cm_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ef7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(true_labels=test_sentiments, predicted_labels=predicted_sentiments,classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd89395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
