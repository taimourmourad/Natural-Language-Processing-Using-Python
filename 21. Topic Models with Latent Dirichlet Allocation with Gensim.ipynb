{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nips12raw_str602', <http.client.HTTPMessage at 0x7f86860fb490>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf nips12raw_str602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orig', 'nips06', 'nips08', 'nips00', 'nips04', 'idx', 'nips02', 'README_yann', 'nips10', 'nips11', 'nips01', 'nips03', 'nips09', 'nips12', 'nips07', 'MATLAB_NOTES', 'nips05', 'RAW_DATA_NOTES']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['nips{0:02}'.format(i) for i in range(0, 13)]\n",
    "# Read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:#seperate 'em with /\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, it looks like the OCR hasn’t worked perfectly and we have\n",
    "some missing characters here and there. This is expected, but also makes this task more\n",
    "challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804 \n",
      "INTRODUCTION TO A SYSTEM FOR IMPLEMENTING NEURAL NET \n",
      "CONNECTIONS ON SIMD ARCHITECTURES \n",
      "Sherryl Tomboulian \n",
      "Institute for Computer Applications in Science and Engineering \n",
      "NASA Langley Research Center, Hampton VA 23665 \n",
      "ABSTRACT \n",
      "Neural networks have attracted much interest recently, and using parallel \n",
      "architectures to simulate neural networks is a natural and necessary applica- \n",
      "tion. The SIMD model of parallel computation is chosen, because systems of \n",
      "this type can be built with large numbers of processing elements. However, \n",
      "such systems are not naturally suited to generalized communication. A method \n",
      "is proposed that allows an implementation of neural network connections on \n",
      "massively parallel SIMD architectures. The key to this system is an algorithm \n",
      "that allows the formation of arbitrary connections between the 'neurons '. A \n",
      "feature is the ability to add new connections quickly. It also has error recov- \n",
      "ery ability and is robust over a variety of network topologies. Si\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')#any word\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]# word tokenization\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'system', 'implementing', 'neural', 'net', 'connection', 'simd', 'architecture', 'sherryl', 'tomboulian', 'institute', 'computer', 'application', 'science', 'engineering', 'nasa', 'langley', 'research', 'center', 'hampton', 'va', 'abstract', 'neural', 'network', 'attracted', 'much', 'interest', 'recently', 'using', 'parallel', 'architecture', 'simulate', 'neural', 'network', 'natural', 'necessary', 'applica', 'tion', 'simd', 'model', 'parallel', 'computation', 'chosen', 'system', 'type', 'built', 'large', 'number', 'processing', 'element']\n"
     ]
    }
   ],
   "source": [
    "# Viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start building topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let’s get started by looking at ways to generate phrases with\n",
    "influential bi-grams and remove some terms that may not be useful before feature\n",
    "engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Featuer Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feature engineering and vectorization, we want to extract some useful bi-gram\n",
    "based phrases from our research papers and remove some unnecessary terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'system', 'implementing', 'neural_net', 'connection', 'simd', 'architecture', 'sherryl', 'tomboulian', 'institute', 'computer', 'application', 'science_engineering', 'nasa', 'langley', 'research_center', 'hampton', 'va', 'abstract', 'neural_network', 'attracted', 'much', 'interest', 'recently', 'using', 'parallel', 'architecture', 'simulate', 'neural_network', 'natural', 'necessary', 'applica', 'tion', 'simd', 'model', 'parallel', 'computation', 'chosen', 'system', 'type', 'built', 'large', 'number', 'processing', 'element', 'however', 'system', 'naturally', 'suited', 'generalized']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter='_') # higher threshold fewer phrases.\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_papers[0]][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s generate phrases for all our tokenized research papers and build a vocabulary\n",
    "that will help us obtain a unique term/phrase to number mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings:  [(0, '8o6'), (1, 'a2'), (2, 'aasp'), (3, 'ability'), (4, 'able'), (5, 'abstract'), (6, 'aca'), (7, 'according'), (8, 'achieve'), (9, 'achieved'), (10, 'acm'), (11, 'act'), (12, 'activate'), (13, 'activated'), (14, 'active')]\n",
      "Total Vocabulary Size:  78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# Create a dictionary representationi of the docuemnts:\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings: ', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a lot of unique phrases in our corpus of research papers,\n",
    "based on the preceding output. Several of these terms are not very useful since they are\n",
    "specific to a paper or even a paragraph in a research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is time to prune\n",
    "our vocabulary and start removing terms. Leveraging document frequency is a great way\n",
    "to achieve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size:  7756\n"
     ]
    }
   ],
   "source": [
    "# fitler out words that occur less than 20 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding\n",
    "different themes and topics and not recurring themes. Hence, this suits our scenario\n",
    "perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now perform feature engineering by leveraging a simple Bag of Words\n",
    "model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 7), (3, 1), (4, 3), (12, 1), (14, 1), (15, 1), (17, 2), (18, 1), (19, 1), (30, 2), (31, 2), (35, 1), (36, 1), (43, 1), (45, 1), (54, 1), (61, 3), (62, 2), (75, 3), (81, 20), (85, 1), (87, 2), (91, 1), (94, 1), (110, 1), (111, 1), (112, 1), (116, 25), (123, 2), (124, 2), (127, 1), (128, 1), (138, 1), (145, 5), (146, 1), (147, 12), (148, 1), (149, 1), (153, 5), (154, 5), (158, 1), (165, 2), (173, 1), (183, 1), (184, 4), (188, 2), (193, 5), (202, 2), (203, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ability', 2), ('able', 7), ('abstract', 1), ('according', 3), ('activity', 1), ('add', 1), ('added', 1), ('addition', 2), ('additional', 1), ('address', 1), ('allow', 2), ('allows', 2), ('always', 1), ('american_institute', 1), ('appears', 1), ('application', 1), ('arranged', 1), ('assume', 3), ('assumed', 2), ('behavior', 3), ('bit', 20), ('bounded', 1), ('brain', 2), ('build', 1), ('called', 1), ('choosing', 1), ('chosen', 1), ('class', 1), ('code', 25), ('communication', 2), ('competitive', 2), ('complex', 1), ('complexity', 1), ('conclusion', 1), ('connected', 5), ('connecting', 1), ('connection', 12), ('consecutive', 1), ('consider', 1), ('constant', 5), ('constraint', 5), ('contain', 1), ('correct', 2), ('currently', 1), ('design', 1), ('designed', 4), ('determined', 2), ('difficult', 5), ('distance', 2), ('distributed', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Viewing actual terms and their counts\n",
    "print([(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers:  1740\n"
     ]
    }
   ],
   "source": [
    "# total papers in the corpus\n",
    "print('Total number of papers: ', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our documents are now processed and have a good enough representation with the\n",
    "Bag of Words model to begin modeling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TOPICS = 10\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto', \n",
    "                                   random_state=42, iterations=500, num_topics=TOTAL_TOPICS, passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the topics in our trained topic model is quite easy and we can generate them\n",
    "with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.011*\"circuit\" + 0.010*\"signal\" + 0.010*\"neuron\" + 0.009*\"chip\" + 0.008*\"motion\" + 0.007*\"current\" + 0.007*\"voltage\" + 0.006*\"analog\" + 0.005*\"frequency\" + 0.005*\"neural\" + 0.005*\"filter\" + 0.004*\"response\" + 0.004*\"noise\" + 0.004*\"channel\" + 0.004*\"processing\" + 0.003*\"cell\" + 0.003*\"velocity\" + 0.003*\"implementation\" + 0.003*\"sound\" + 0.003*\"synapse\"\n",
      "\n",
      "Topic #2:\n",
      "0.019*\"neuron\" + 0.019*\"cell\" + 0.010*\"response\" + 0.010*\"stimulus\" + 0.009*\"activity\" + 0.008*\"pattern\" + 0.007*\"unit\" + 0.005*\"layer\" + 0.005*\"visual\" + 0.005*\"synaptic\" + 0.005*\"cortical\" + 0.004*\"connection\" + 0.004*\"firing\" + 0.004*\"effect\" + 0.004*\"et_al\" + 0.004*\"neural\" + 0.004*\"cortex\" + 0.004*\"simulation\" + 0.004*\"map\" + 0.004*\"spike\"\n",
      "\n",
      "Topic #3:\n",
      "0.028*\"state\" + 0.014*\"control\" + 0.011*\"action\" + 0.007*\"step\" + 0.007*\"policy\" + 0.007*\"trajectory\" + 0.006*\"task\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"optimal\" + 0.005*\"environment\" + 0.004*\"robot\" + 0.004*\"dynamic\" + 0.004*\"goal\" + 0.003*\"reward\" + 0.003*\"position\" + 0.003*\"change\" + 0.003*\"agent\" + 0.003*\"td\" + 0.003*\"current\"\n",
      "\n",
      "Topic #4:\n",
      "0.017*\"node\" + 0.013*\"structure\" + 0.009*\"cluster\" + 0.007*\"tree\" + 0.007*\"graph\" + 0.006*\"clustering\" + 0.006*\"variable\" + 0.006*\"representation\" + 0.006*\"map\" + 0.006*\"vector\" + 0.005*\"level\" + 0.004*\"edge\" + 0.004*\"local\" + 0.004*\"constraint\" + 0.004*\"rule\" + 0.004*\"mapping\" + 0.004*\"region\" + 0.003*\"object\" + 0.003*\"approximation\" + 0.003*\"part\"\n",
      "\n",
      "Topic #5:\n",
      "0.007*\"rate\" + 0.007*\"variable\" + 0.006*\"spike\" + 0.006*\"sample\" + 0.005*\"estimate\" + 0.005*\"signal\" + 0.005*\"feature\" + 0.005*\"channel\" + 0.005*\"average\" + 0.004*\"component\" + 0.004*\"probability\" + 0.003*\"noise\" + 0.003*\"missing\" + 0.003*\"eeg\" + 0.003*\"search\" + 0.003*\"risk\" + 0.003*\"step\" + 0.003*\"density\" + 0.003*\"classification\" + 0.003*\"solution\"\n",
      "\n",
      "Topic #6:\n",
      "0.014*\"unit\" + 0.012*\"training\" + 0.009*\"pattern\" + 0.009*\"word\" + 0.008*\"task\" + 0.006*\"rule\" + 0.006*\"trained\" + 0.006*\"recognition\" + 0.005*\"layer\" + 0.005*\"feature\" + 0.005*\"sequence\" + 0.005*\"architecture\" + 0.005*\"representation\" + 0.004*\"memory\" + 0.004*\"net\" + 0.004*\"hidden_unit\" + 0.004*\"activation\" + 0.004*\"speech\" + 0.004*\"character\" + 0.003*\"level\"\n",
      "\n",
      "Topic #7:\n",
      "0.012*\"training\" + 0.011*\"unit\" + 0.006*\"vector\" + 0.006*\"layer\" + 0.006*\"net\" + 0.006*\"state\" + 0.006*\"hidden_unit\" + 0.005*\"linear\" + 0.004*\"neuron\" + 0.004*\"architecture\" + 0.004*\"recurrent\" + 0.003*\"class\" + 0.003*\"node\" + 0.003*\"hmm\" + 0.003*\"threshold\" + 0.003*\"activation\" + 0.003*\"gradient\" + 0.003*\"mlp\" + 0.003*\"pattern\" + 0.003*\"nonlinear\"\n",
      "\n",
      "Topic #8:\n",
      "0.037*\"image\" + 0.015*\"feature\" + 0.011*\"object\" + 0.007*\"pixel\" + 0.006*\"face\" + 0.006*\"vector\" + 0.006*\"representation\" + 0.005*\"transformation\" + 0.005*\"recognition\" + 0.005*\"view\" + 0.004*\"local\" + 0.004*\"visual\" + 0.004*\"distance\" + 0.004*\"class\" + 0.004*\"linear\" + 0.004*\"classification\" + 0.003*\"scale\" + 0.003*\"training\" + 0.003*\"filter\" + 0.003*\"shape\"\n",
      "\n",
      "Topic #9:\n",
      "0.012*\"class\" + 0.009*\"training\" + 0.008*\"classifier\" + 0.006*\"classification\" + 0.006*\"probability\" + 0.006*\"sample\" + 0.005*\"distribution\" + 0.005*\"kernel\" + 0.005*\"bound\" + 0.005*\"test\" + 0.004*\"prediction\" + 0.004*\"let\" + 0.004*\"size\" + 0.004*\"machine\" + 0.004*\"estimate\" + 0.004*\"loss\" + 0.004*\"linear\" + 0.004*\"training_set\" + 0.004*\"vector\" + 0.004*\"regression\"\n",
      "\n",
      "Topic #10:\n",
      "0.008*\"distribution\" + 0.007*\"equation\" + 0.007*\"state\" + 0.007*\"matrix\" + 0.007*\"vector\" + 0.006*\"noise\" + 0.005*\"dynamic\" + 0.005*\"gaussian\" + 0.004*\"approximation\" + 0.004*\"solution\" + 0.004*\"density\" + 0.004*\"variable\" + 0.003*\"probability\" + 0.003*\"component\" + 0.003*\"theory\" + 0.003*\"linear\" + 0.003*\"eq\" + 0.003*\"step\" + 0.003*\"prior\" + 0.003*\"rule\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can also view the overall mean coherence score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -1.0961562166070156\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s\n",
    "now look at the output of our LDA topic model in an easier to understand format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One\n",
    "way is to visualize the topics as tuples of terms and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('distribution', 0.008), ('equation', 0.007), ('state', 0.007), ('matrix', 0.007), ('vector', 0.007), ('noise', 0.006), ('dynamic', 0.005), ('gaussian', 0.005), ('approximation', 0.004), ('solution', 0.004), ('density', 0.004), ('variable', 0.004), ('probability', 0.003), ('component', 0.003), ('theory', 0.003), ('linear', 0.003), ('eq', 0.003), ('step', 0.003), ('prior', 0.003), ('rule', 0.003)]\n",
      "\n",
      "Topic #2:\n",
      "[('class', 0.012), ('training', 0.009), ('classifier', 0.008), ('classification', 0.006), ('probability', 0.006), ('sample', 0.006), ('distribution', 0.005), ('kernel', 0.005), ('bound', 0.005), ('test', 0.005), ('prediction', 0.004), ('let', 0.004), ('size', 0.004), ('machine', 0.004), ('estimate', 0.004), ('loss', 0.004), ('linear', 0.004), ('training_set', 0.004), ('vector', 0.004), ('regression', 0.004)]\n",
      "\n",
      "Topic #3:\n",
      "[('neuron', 0.019), ('cell', 0.019), ('response', 0.01), ('stimulus', 0.01), ('activity', 0.009), ('pattern', 0.008), ('unit', 0.007), ('layer', 0.005), ('visual', 0.005), ('synaptic', 0.005), ('cortical', 0.005), ('connection', 0.004), ('firing', 0.004), ('effect', 0.004), ('et_al', 0.004), ('neural', 0.004), ('cortex', 0.004), ('simulation', 0.004), ('map', 0.004), ('spike', 0.004)]\n",
      "\n",
      "Topic #4:\n",
      "[('node', 0.017), ('structure', 0.013), ('cluster', 0.009), ('tree', 0.007), ('graph', 0.007), ('clustering', 0.006), ('variable', 0.006), ('representation', 0.006), ('map', 0.006), ('vector', 0.006), ('level', 0.005), ('edge', 0.004), ('local', 0.004), ('constraint', 0.004), ('rule', 0.004), ('mapping', 0.004), ('region', 0.004), ('object', 0.003), ('approximation', 0.003), ('part', 0.003)]\n",
      "\n",
      "Topic #5:\n",
      "[('image', 0.037), ('feature', 0.015), ('object', 0.011), ('pixel', 0.007), ('face', 0.006), ('vector', 0.006), ('representation', 0.006), ('transformation', 0.005), ('recognition', 0.005), ('view', 0.005), ('local', 0.004), ('visual', 0.004), ('distance', 0.004), ('class', 0.004), ('linear', 0.004), ('classification', 0.004), ('scale', 0.003), ('training', 0.003), ('filter', 0.003), ('shape', 0.003)]\n",
      "\n",
      "Topic #6:\n",
      "[('unit', 0.014), ('training', 0.012), ('pattern', 0.009), ('word', 0.009), ('task', 0.008), ('rule', 0.006), ('trained', 0.006), ('recognition', 0.006), ('layer', 0.005), ('feature', 0.005), ('sequence', 0.005), ('architecture', 0.005), ('representation', 0.005), ('memory', 0.004), ('net', 0.004), ('hidden_unit', 0.004), ('activation', 0.004), ('speech', 0.004), ('character', 0.004), ('level', 0.003)]\n",
      "\n",
      "Topic #7:\n",
      "[('state', 0.028), ('control', 0.014), ('action', 0.011), ('step', 0.007), ('policy', 0.007), ('trajectory', 0.007), ('task', 0.006), ('controller', 0.006), ('reinforcement_learning', 0.005), ('optimal', 0.005), ('environment', 0.005), ('robot', 0.004), ('dynamic', 0.004), ('goal', 0.004), ('reward', 0.003), ('position', 0.003), ('change', 0.003), ('agent', 0.003), ('td', 0.003), ('current', 0.003)]\n",
      "\n",
      "Topic #8:\n",
      "[('circuit', 0.011), ('signal', 0.01), ('neuron', 0.01), ('chip', 0.009), ('motion', 0.008), ('current', 0.007), ('voltage', 0.007), ('analog', 0.006), ('frequency', 0.005), ('neural', 0.005), ('filter', 0.005), ('response', 0.004), ('noise', 0.004), ('channel', 0.004), ('processing', 0.004), ('cell', 0.003), ('velocity', 0.003), ('implementation', 0.003), ('sound', 0.003), ('synapse', 0.003)]\n",
      "\n",
      "Topic #9:\n",
      "[('training', 0.012), ('unit', 0.011), ('vector', 0.006), ('layer', 0.006), ('net', 0.006), ('state', 0.006), ('hidden_unit', 0.006), ('linear', 0.005), ('neuron', 0.004), ('architecture', 0.004), ('recurrent', 0.004), ('class', 0.003), ('node', 0.003), ('hmm', 0.003), ('threshold', 0.003), ('activation', 0.003), ('gradient', 0.003), ('mlp', 0.003), ('pattern', 0.003), ('nonlinear', 0.003)]\n",
      "\n",
      "Topic #10:\n",
      "[('rate', 0.007), ('variable', 0.007), ('spike', 0.006), ('sample', 0.006), ('estimate', 0.005), ('signal', 0.005), ('feature', 0.005), ('channel', 0.005), ('average', 0.005), ('component', 0.004), ('probability', 0.004), ('noise', 0.003), ('missing', 0.003), ('eeg', 0.003), ('search', 0.003), ('risk', 0.003), ('step', 0.003), ('density', 0.003), ('classification', 0.003), ('solution', 0.003)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the topics as a list of terms without the weights when we want to\n",
    "understand the context or theme conveyed by each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['distribution', 'equation', 'state', 'matrix', 'vector', 'noise', 'dynamic', 'gaussian', 'approximation', 'solution', 'density', 'variable', 'probability', 'component', 'theory', 'linear', 'eq', 'step', 'prior', 'rule']\n",
      "\n",
      "Topic #2:\n",
      "['class', 'training', 'classifier', 'classification', 'probability', 'sample', 'distribution', 'kernel', 'bound', 'test', 'prediction', 'let', 'size', 'machine', 'estimate', 'loss', 'linear', 'training_set', 'vector', 'regression']\n",
      "\n",
      "Topic #3:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'activity', 'pattern', 'unit', 'layer', 'visual', 'synaptic', 'cortical', 'connection', 'firing', 'effect', 'et_al', 'neural', 'cortex', 'simulation', 'map', 'spike']\n",
      "\n",
      "Topic #4:\n",
      "['node', 'structure', 'cluster', 'tree', 'graph', 'clustering', 'variable', 'representation', 'map', 'vector', 'level', 'edge', 'local', 'constraint', 'rule', 'mapping', 'region', 'object', 'approximation', 'part']\n",
      "\n",
      "Topic #5:\n",
      "['image', 'feature', 'object', 'pixel', 'face', 'vector', 'representation', 'transformation', 'recognition', 'view', 'local', 'visual', 'distance', 'class', 'linear', 'classification', 'scale', 'training', 'filter', 'shape']\n",
      "\n",
      "Topic #6:\n",
      "['unit', 'training', 'pattern', 'word', 'task', 'rule', 'trained', 'recognition', 'layer', 'feature', 'sequence', 'architecture', 'representation', 'memory', 'net', 'hidden_unit', 'activation', 'speech', 'character', 'level']\n",
      "\n",
      "Topic #7:\n",
      "['state', 'control', 'action', 'step', 'policy', 'trajectory', 'task', 'controller', 'reinforcement_learning', 'optimal', 'environment', 'robot', 'dynamic', 'goal', 'reward', 'position', 'change', 'agent', 'td', 'current']\n",
      "\n",
      "Topic #8:\n",
      "['circuit', 'signal', 'neuron', 'chip', 'motion', 'current', 'voltage', 'analog', 'frequency', 'neural', 'filter', 'response', 'noise', 'channel', 'processing', 'cell', 'velocity', 'implementation', 'sound', 'synapse']\n",
      "\n",
      "Topic #9:\n",
      "['training', 'unit', 'vector', 'layer', 'net', 'state', 'hidden_unit', 'linear', 'neuron', 'architecture', 'recurrent', 'class', 'node', 'hmm', 'threshold', 'activation', 'gradient', 'mlp', 'pattern', 'nonlinear']\n",
      "\n",
      "Topic #10:\n",
      "['rate', 'variable', 'spike', 'sample', 'estimate', 'signal', 'feature', 'channel', 'average', 'component', 'probability', 'noise', 'missing', 'eeg', 'search', 'risk', 'step', 'density', 'classification', 'solution']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation our Model Using the Perplexity and Coherence metircs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.4525712015965685\n",
      "Avg. Coherence Score (UMass): -1.0961562166070158\n",
      "Model Perplexity: -7.797277885229535\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary, \n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary, \n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Tuning: Finding the Optimal Number of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an iterative approach and build several models with differing numbers of topics and select the one that has the highest coherence score. To implement this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):#2: 11\n",
    "        TOTAL_TOPICS = 10\n",
    "        lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto', \n",
    "                                   random_state=42, iterations=500, num_topics=TOTAL_TOPICS, passes=20, eval_every=None)\n",
    "        cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=corpus, \n",
    "                                                                     texts=texts, dictionary=dictionary, \n",
    "                                                                     coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(lda_model)\n",
    "    \n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [1:59:17<00:00, 246.82s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n",
    "                                                               dictionary=dictionary, start_topic_count=2,\n",
    "                                                               end_topic_count=30, step=1, cpus=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the topic models based on the coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Coherence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>0.4526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Topics  Coherence Score\n",
       "0                  2           0.4526\n",
       "15                17           0.4526\n",
       "27                29           0.4526\n",
       "26                28           0.4526\n",
       "25                27           0.4526\n",
       "24                26           0.4526\n",
       "23                25           0.4526\n",
       "22                24           0.4526\n",
       "21                23           0.4526\n",
       "20                22           0.4526"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1), 'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot a graph showing the number of topics per model and their corresponding coherence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic model tuning the number of topics vs. coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(2, 31, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the best model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 20].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s view all the 20 topics generated by our selected best model, similar to our previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['circuit', 'signal', 'neuron', 'chip', 'motion', 'current', 'voltage', 'analog', 'frequency', 'neural', 'filter', 'response', 'noise', 'channel', 'processing', 'cell', 'velocity', 'implementation', 'sound', 'synapse']\n",
      "\n",
      "Topic #2:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'activity', 'pattern', 'unit', 'layer', 'visual', 'synaptic', 'cortical', 'connection', 'firing', 'effect', 'et_al', 'neural', 'cortex', 'simulation', 'map', 'spike']\n",
      "\n",
      "Topic #3:\n",
      "['state', 'control', 'action', 'step', 'policy', 'trajectory', 'task', 'controller', 'reinforcement_learning', 'optimal', 'environment', 'robot', 'dynamic', 'goal', 'reward', 'position', 'change', 'agent', 'td', 'current']\n",
      "\n",
      "Topic #4:\n",
      "['node', 'structure', 'cluster', 'tree', 'graph', 'clustering', 'variable', 'representation', 'map', 'vector', 'level', 'edge', 'local', 'constraint', 'rule', 'mapping', 'region', 'object', 'approximation', 'part']\n",
      "\n",
      "Topic #5:\n",
      "['rate', 'variable', 'spike', 'sample', 'estimate', 'signal', 'feature', 'channel', 'average', 'component', 'probability', 'noise', 'missing', 'eeg', 'search', 'risk', 'step', 'density', 'classification', 'solution']\n",
      "\n",
      "Topic #6:\n",
      "['unit', 'training', 'pattern', 'word', 'task', 'rule', 'trained', 'recognition', 'layer', 'feature', 'sequence', 'architecture', 'representation', 'memory', 'net', 'hidden_unit', 'activation', 'speech', 'character', 'level']\n",
      "\n",
      "Topic #7:\n",
      "['training', 'unit', 'vector', 'layer', 'net', 'state', 'hidden_unit', 'linear', 'neuron', 'architecture', 'recurrent', 'class', 'node', 'hmm', 'threshold', 'activation', 'gradient', 'mlp', 'pattern', 'nonlinear']\n",
      "\n",
      "Topic #8:\n",
      "['image', 'feature', 'object', 'pixel', 'face', 'vector', 'representation', 'transformation', 'recognition', 'view', 'local', 'visual', 'distance', 'class', 'linear', 'classification', 'scale', 'training', 'filter', 'shape']\n",
      "\n",
      "Topic #9:\n",
      "['class', 'training', 'classifier', 'classification', 'probability', 'sample', 'distribution', 'kernel', 'bound', 'test', 'prediction', 'let', 'size', 'machine', 'estimate', 'loss', 'linear', 'training_set', 'vector', 'regression']\n",
      "\n",
      "Topic #10:\n",
      "['distribution', 'equation', 'state', 'matrix', 'vector', 'noise', 'dynamic', 'gaussian', 'approximation', 'solution', 'density', 'variable', 'probability', 'component', 'theory', 'linear', 'eq', 'step', 'prior', 'rule']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "      <th>Topic 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Term1</th>\n",
       "      <td>circuit</td>\n",
       "      <td>neuron</td>\n",
       "      <td>state</td>\n",
       "      <td>node</td>\n",
       "      <td>rate</td>\n",
       "      <td>unit</td>\n",
       "      <td>training</td>\n",
       "      <td>image</td>\n",
       "      <td>class</td>\n",
       "      <td>distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term2</th>\n",
       "      <td>signal</td>\n",
       "      <td>cell</td>\n",
       "      <td>control</td>\n",
       "      <td>structure</td>\n",
       "      <td>variable</td>\n",
       "      <td>training</td>\n",
       "      <td>unit</td>\n",
       "      <td>feature</td>\n",
       "      <td>training</td>\n",
       "      <td>equation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term3</th>\n",
       "      <td>neuron</td>\n",
       "      <td>response</td>\n",
       "      <td>action</td>\n",
       "      <td>cluster</td>\n",
       "      <td>spike</td>\n",
       "      <td>pattern</td>\n",
       "      <td>vector</td>\n",
       "      <td>object</td>\n",
       "      <td>classifier</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term4</th>\n",
       "      <td>chip</td>\n",
       "      <td>stimulus</td>\n",
       "      <td>step</td>\n",
       "      <td>tree</td>\n",
       "      <td>sample</td>\n",
       "      <td>word</td>\n",
       "      <td>layer</td>\n",
       "      <td>pixel</td>\n",
       "      <td>classification</td>\n",
       "      <td>matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term5</th>\n",
       "      <td>motion</td>\n",
       "      <td>activity</td>\n",
       "      <td>policy</td>\n",
       "      <td>graph</td>\n",
       "      <td>estimate</td>\n",
       "      <td>task</td>\n",
       "      <td>net</td>\n",
       "      <td>face</td>\n",
       "      <td>probability</td>\n",
       "      <td>vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term6</th>\n",
       "      <td>current</td>\n",
       "      <td>pattern</td>\n",
       "      <td>trajectory</td>\n",
       "      <td>clustering</td>\n",
       "      <td>signal</td>\n",
       "      <td>rule</td>\n",
       "      <td>state</td>\n",
       "      <td>vector</td>\n",
       "      <td>sample</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term7</th>\n",
       "      <td>voltage</td>\n",
       "      <td>unit</td>\n",
       "      <td>task</td>\n",
       "      <td>variable</td>\n",
       "      <td>feature</td>\n",
       "      <td>trained</td>\n",
       "      <td>hidden_unit</td>\n",
       "      <td>representation</td>\n",
       "      <td>distribution</td>\n",
       "      <td>dynamic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term8</th>\n",
       "      <td>analog</td>\n",
       "      <td>layer</td>\n",
       "      <td>controller</td>\n",
       "      <td>representation</td>\n",
       "      <td>channel</td>\n",
       "      <td>recognition</td>\n",
       "      <td>linear</td>\n",
       "      <td>transformation</td>\n",
       "      <td>kernel</td>\n",
       "      <td>gaussian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term9</th>\n",
       "      <td>frequency</td>\n",
       "      <td>visual</td>\n",
       "      <td>reinforcement_learning</td>\n",
       "      <td>map</td>\n",
       "      <td>average</td>\n",
       "      <td>layer</td>\n",
       "      <td>neuron</td>\n",
       "      <td>recognition</td>\n",
       "      <td>bound</td>\n",
       "      <td>approximation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term10</th>\n",
       "      <td>neural</td>\n",
       "      <td>synaptic</td>\n",
       "      <td>optimal</td>\n",
       "      <td>vector</td>\n",
       "      <td>component</td>\n",
       "      <td>feature</td>\n",
       "      <td>architecture</td>\n",
       "      <td>view</td>\n",
       "      <td>test</td>\n",
       "      <td>solution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term11</th>\n",
       "      <td>filter</td>\n",
       "      <td>cortical</td>\n",
       "      <td>environment</td>\n",
       "      <td>level</td>\n",
       "      <td>probability</td>\n",
       "      <td>sequence</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>local</td>\n",
       "      <td>prediction</td>\n",
       "      <td>density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term12</th>\n",
       "      <td>response</td>\n",
       "      <td>connection</td>\n",
       "      <td>robot</td>\n",
       "      <td>edge</td>\n",
       "      <td>noise</td>\n",
       "      <td>architecture</td>\n",
       "      <td>class</td>\n",
       "      <td>visual</td>\n",
       "      <td>let</td>\n",
       "      <td>variable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term13</th>\n",
       "      <td>noise</td>\n",
       "      <td>firing</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>local</td>\n",
       "      <td>missing</td>\n",
       "      <td>representation</td>\n",
       "      <td>node</td>\n",
       "      <td>distance</td>\n",
       "      <td>size</td>\n",
       "      <td>probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term14</th>\n",
       "      <td>channel</td>\n",
       "      <td>effect</td>\n",
       "      <td>goal</td>\n",
       "      <td>constraint</td>\n",
       "      <td>eeg</td>\n",
       "      <td>memory</td>\n",
       "      <td>hmm</td>\n",
       "      <td>class</td>\n",
       "      <td>machine</td>\n",
       "      <td>component</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term15</th>\n",
       "      <td>processing</td>\n",
       "      <td>et_al</td>\n",
       "      <td>reward</td>\n",
       "      <td>rule</td>\n",
       "      <td>search</td>\n",
       "      <td>net</td>\n",
       "      <td>threshold</td>\n",
       "      <td>linear</td>\n",
       "      <td>estimate</td>\n",
       "      <td>theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term16</th>\n",
       "      <td>cell</td>\n",
       "      <td>neural</td>\n",
       "      <td>position</td>\n",
       "      <td>mapping</td>\n",
       "      <td>risk</td>\n",
       "      <td>hidden_unit</td>\n",
       "      <td>activation</td>\n",
       "      <td>classification</td>\n",
       "      <td>loss</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term17</th>\n",
       "      <td>velocity</td>\n",
       "      <td>cortex</td>\n",
       "      <td>change</td>\n",
       "      <td>region</td>\n",
       "      <td>step</td>\n",
       "      <td>activation</td>\n",
       "      <td>gradient</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term18</th>\n",
       "      <td>implementation</td>\n",
       "      <td>simulation</td>\n",
       "      <td>agent</td>\n",
       "      <td>object</td>\n",
       "      <td>density</td>\n",
       "      <td>speech</td>\n",
       "      <td>mlp</td>\n",
       "      <td>training</td>\n",
       "      <td>training_set</td>\n",
       "      <td>step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term19</th>\n",
       "      <td>sound</td>\n",
       "      <td>map</td>\n",
       "      <td>td</td>\n",
       "      <td>approximation</td>\n",
       "      <td>classification</td>\n",
       "      <td>character</td>\n",
       "      <td>pattern</td>\n",
       "      <td>filter</td>\n",
       "      <td>vector</td>\n",
       "      <td>prior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term20</th>\n",
       "      <td>synapse</td>\n",
       "      <td>spike</td>\n",
       "      <td>current</td>\n",
       "      <td>part</td>\n",
       "      <td>solution</td>\n",
       "      <td>level</td>\n",
       "      <td>nonlinear</td>\n",
       "      <td>shape</td>\n",
       "      <td>regression</td>\n",
       "      <td>rule</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Topic 1     Topic 2                 Topic 3         Topic 4  \\\n",
       "Term1          circuit      neuron                   state            node   \n",
       "Term2           signal        cell                 control       structure   \n",
       "Term3           neuron    response                  action         cluster   \n",
       "Term4             chip    stimulus                    step            tree   \n",
       "Term5           motion    activity                  policy           graph   \n",
       "Term6          current     pattern              trajectory      clustering   \n",
       "Term7          voltage        unit                    task        variable   \n",
       "Term8           analog       layer              controller  representation   \n",
       "Term9        frequency      visual  reinforcement_learning             map   \n",
       "Term10          neural    synaptic                 optimal          vector   \n",
       "Term11          filter    cortical             environment           level   \n",
       "Term12        response  connection                   robot            edge   \n",
       "Term13           noise      firing                 dynamic           local   \n",
       "Term14         channel      effect                    goal      constraint   \n",
       "Term15      processing       et_al                  reward            rule   \n",
       "Term16            cell      neural                position         mapping   \n",
       "Term17        velocity      cortex                  change          region   \n",
       "Term18  implementation  simulation                   agent          object   \n",
       "Term19           sound         map                      td   approximation   \n",
       "Term20         synapse       spike                 current            part   \n",
       "\n",
       "               Topic 5         Topic 6       Topic 7         Topic 8  \\\n",
       "Term1             rate            unit      training           image   \n",
       "Term2         variable        training          unit         feature   \n",
       "Term3            spike         pattern        vector          object   \n",
       "Term4           sample            word         layer           pixel   \n",
       "Term5         estimate            task           net            face   \n",
       "Term6           signal            rule         state          vector   \n",
       "Term7          feature         trained   hidden_unit  representation   \n",
       "Term8          channel     recognition        linear  transformation   \n",
       "Term9          average           layer        neuron     recognition   \n",
       "Term10       component         feature  architecture            view   \n",
       "Term11     probability        sequence     recurrent           local   \n",
       "Term12           noise    architecture         class          visual   \n",
       "Term13         missing  representation          node        distance   \n",
       "Term14             eeg          memory           hmm           class   \n",
       "Term15          search             net     threshold          linear   \n",
       "Term16            risk     hidden_unit    activation  classification   \n",
       "Term17            step      activation      gradient           scale   \n",
       "Term18         density          speech           mlp        training   \n",
       "Term19  classification       character       pattern          filter   \n",
       "Term20        solution           level     nonlinear           shape   \n",
       "\n",
       "               Topic 9       Topic 10  \n",
       "Term1            class   distribution  \n",
       "Term2         training       equation  \n",
       "Term3       classifier          state  \n",
       "Term4   classification         matrix  \n",
       "Term5      probability         vector  \n",
       "Term6           sample          noise  \n",
       "Term7     distribution        dynamic  \n",
       "Term8           kernel       gaussian  \n",
       "Term9            bound  approximation  \n",
       "Term10            test       solution  \n",
       "Term11      prediction        density  \n",
       "Term12             let       variable  \n",
       "Term13            size    probability  \n",
       "Term14         machine      component  \n",
       "Term15        estimate         theory  \n",
       "Term16            loss         linear  \n",
       "Term17          linear             eq  \n",
       "Term18    training_set           step  \n",
       "Term19          vector          prior  \n",
       "Term20      regression           rule  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.DataFrame([[term for term, wt in topic] \n",
    "                              for topic in topics], \n",
    "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another easy way to view the topics is to create a topic-term dataframe, whereby each topic is represented in a row with the terms of the topic being represented as a comma-separated string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-8a50780ba5c5>:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>neuron, cell, response, stimulus, activity, pattern, unit, layer, visual, synaptic, cortical, connection, firing, effect, et_al, neural, cortex, simulation, map, spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>state, control, action, step, policy, trajectory, task, controller, reinforcement_learning, optimal, environment, robot, dynamic, goal, reward, position, change, agent, td, current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>node, structure, cluster, tree, graph, clustering, variable, representation, map, vector, level, edge, local, constraint, rule, mapping, region, object, approximation, part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>rate, variable, spike, sample, estimate, signal, feature, channel, average, component, probability, noise, missing, eeg, search, risk, step, density, classification, solution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       Terms per Topic\n",
       "Topic1   circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse             \n",
       "Topic2   neuron, cell, response, stimulus, activity, pattern, unit, layer, visual, synaptic, cortical, connection, firing, effect, et_al, neural, cortex, simulation, map, spike                      \n",
       "Topic3   state, control, action, step, policy, trajectory, task, controller, reinforcement_learning, optimal, environment, robot, dynamic, goal, reward, position, change, agent, td, current         \n",
       "Topic4   node, structure, cluster, tree, graph, clustering, variable, representation, map, vector, level, edge, local, constraint, rule, mapping, region, object, approximation, part                 \n",
       "Topic5   rate, variable, spike, sample, estimate, signal, feature, channel, average, component, probability, noise, missing, eeg, search, risk, step, density, classification, solution               \n",
       "Topic6   unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level      \n",
       "Topic7   training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear                  \n",
       "Topic8   image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape       \n",
       "Topic9   class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression\n",
       "Topic10  distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule           "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_lda_model.num_topics+1)]\n",
    "                         )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Topic Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.46751153),\n",
       " (9, 0.4825014),\n",
       " (9, 0.4648029),\n",
       " (6, 0.8165061),\n",
       " (6, 0.64134735)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_results = best_lda_model[bow_corpus]\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]\n",
    "corpus_topics[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides a plethora of options that can be leveraged to extract useful insights from our corpus of research papers\n",
    "<br>\n",
    "To enable this, we construct a master dataframe that will hold the base statistics, which we use soon to depict different useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominant Topics Distribution Across Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg({\n",
    "                                                'Dominant Topic': {\n",
    "                                                    'Doc Count': np.size,\n",
    "                                                    '% Total Docs': np.size }\n",
    "                                              })\n",
    "topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n",
    "topic_stats_df['% Total Docs'] = topic_stats_df['% Total Docs'].apply(lambda row: round((row*100) / len(papers), 2))\n",
    "topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] for t in range(len(topic_stats_df))]\n",
    "topic_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominant Topics in Specific Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>55.87</td>\n",
       "      <td>image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape</td>\n",
       "      <td>622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>60.42</td>\n",
       "      <td>training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear</td>\n",
       "      <td>534 \\nThe Performance of Convex Set Projection Based Neural Networks \\nRobert J. Marks II, Les E. Atlas, Seho Oh and James A. Ritcey \\nInteractive Systems Design Lab, FT-10 \\nUniversity of Washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>59.42</td>\n",
       "      <td>unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level</td>\n",
       "      <td>652 \\nScaling Properties of Coarse-Coded Symbol Memories \\nRonald Rosenfeld \\nDavid S. Touretzky \\nComputer Science Department \\nCarnegie Mellon University \\nPittsburgh, Pennsylvania 15213 \\nAbstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>7</td>\n",
       "      <td>52.24</td>\n",
       "      <td>training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear</td>\n",
       "      <td>ART2/BP architecture for adaptive estimation of \\ndynamic processes \\nEinar Srheim* \\nDepartment of Computer Science \\nUNIK, Kjeller \\nUniversity of Oslo \\nN-2007 Norway \\nAbstract \\nThe goal has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>6</td>\n",
       "      <td>51.99</td>\n",
       "      <td>unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level</td>\n",
       "      <td>Information Measure Based Skeletonisation \\nSowmya Ramachandran \\nDepartment of Computer Science \\nUniversity of Texas at Austin \\nAustin, TX 78712-1188 \\nLorien Y. Pratt * \\nDepartment of Compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>681</td>\n",
       "      <td>1</td>\n",
       "      <td>91.31</td>\n",
       "      <td>circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse</td>\n",
       "      <td>Analog Cochlear Model for Multiresolution \\nSpeech Analysis \\nWeimin Liu Andreas G. Andreou and Moise H. Goldstein, Jr. \\nDepartment of Electrical and Computer Engineering \\nThe Johns Hopkins Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>733</td>\n",
       "      <td>10</td>\n",
       "      <td>46.46</td>\n",
       "      <td>distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule</td>\n",
       "      <td>Two Iterative Algorithms for Computing \\nthe Singular Value Decomposition from \\nInput[Output Samples \\nTerence D. Sanger \\nJet Propulsion Laboratory \\nMS 303-310 \\n4800 Oak Grove Drive \\nPasadena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>906</td>\n",
       "      <td>9</td>\n",
       "      <td>69.62</td>\n",
       "      <td>class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression</td>\n",
       "      <td>Generalisation in Feedforward Networks \\nAdam Kowalczyk and Herman Ferra \\nTelecom Australia, Research Laboratories \\n770 Blackburn Road, Clayton, Vic. 3168, Australia \\n(a.kowalczyk@trl.oz.au, h....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>10</td>\n",
       "      <td>99.95</td>\n",
       "      <td>distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule</td>\n",
       "      <td>A New Learning Algorithm for Blind \\nSignal Separation \\nS. Amari* \\nUniversity of Tokyo \\nBunkyo-ku, Tokyo 113, JAPAN \\namari @ sat. t. u- tokyo. ac.j p \\nA. Cichocki \\nLab. for Artificial Brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>1622</td>\n",
       "      <td>9</td>\n",
       "      <td>52.45</td>\n",
       "      <td>class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression</td>\n",
       "      <td>Training Data Selection \\nfor Optimal Generalization \\nin Trigonometric Polynomial Networks \\nMasashi Sugiyama*and Hidemitsu Ogawa \\nDepartment of Computer Science, Tokyo Institute of Technology, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document  Dominant Topic  Contribution %  \\\n",
       "9            9               8           55.87   \n",
       "13          13               7           60.42   \n",
       "17          17               6           59.42   \n",
       "392        392               7           52.24   \n",
       "503        503               6           51.99   \n",
       "681        681               1           91.31   \n",
       "733        733              10           46.46   \n",
       "906        906               9           69.62   \n",
       "996        996              10           99.95   \n",
       "1622      1622               9           52.45   \n",
       "\n",
       "                                                                                                                                                                                         Topic Desc  \\\n",
       "9            image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape   \n",
       "13                      training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear   \n",
       "17          unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level   \n",
       "392                     training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear   \n",
       "503         unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level   \n",
       "681                circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse   \n",
       "733              distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule   \n",
       "906   class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression   \n",
       "996              distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule   \n",
       "1622  class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression   \n",
       "\n",
       "                                                                                                                                                                                                        Paper  \n",
       "9     622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...  \n",
       "13    534 \\nThe Performance of Convex Set Projection Based Neural Networks \\nRobert J. Marks II, Les E. Atlas, Seho Oh and James A. Ritcey \\nInteractive Systems Design Lab, FT-10 \\nUniversity of Washing...  \n",
       "17    652 \\nScaling Properties of Coarse-Coded Symbol Memories \\nRonald Rosenfeld \\nDavid S. Touretzky \\nComputer Science Department \\nCarnegie Mellon University \\nPittsburgh, Pennsylvania 15213 \\nAbstr...  \n",
       "392   ART2/BP architecture for adaptive estimation of \\ndynamic processes \\nEinar Srheim* \\nDepartment of Computer Science \\nUNIK, Kjeller \\nUniversity of Oslo \\nN-2007 Norway \\nAbstract \\nThe goal has...  \n",
       "503   Information Measure Based Skeletonisation \\nSowmya Ramachandran \\nDepartment of Computer Science \\nUniversity of Texas at Austin \\nAustin, TX 78712-1188 \\nLorien Y. Pratt * \\nDepartment of Compute...  \n",
       "681   Analog Cochlear Model for Multiresolution \\nSpeech Analysis \\nWeimin Liu Andreas G. Andreou and Moise H. Goldstein, Jr. \\nDepartment of Electrical and Computer Engineering \\nThe Johns Hopkins Uni...  \n",
       "733   Two Iterative Algorithms for Computing \\nthe Singular Value Decomposition from \\nInput[Output Samples \\nTerence D. Sanger \\nJet Propulsion Laboratory \\nMS 303-310 \\n4800 Oak Grove Drive \\nPasadena...  \n",
       "906   Generalisation in Feedforward Networks \\nAdam Kowalczyk and Herman Ferra \\nTelecom Australia, Research Laboratories \\n770 Blackburn Road, Clayton, Vic. 3168, Australia \\n(a.kowalczyk@trl.oz.au, h....  \n",
       "996   A New Learning Algorithm for Blind \\nSignal Separation \\nS. Amari* \\nUniversity of Tokyo \\nBunkyo-ku, Tokyo 113, JAPAN \\namari @ sat. t. u- tokyo. ac.j p \\nA. Cichocki \\nLab. for Artificial Brain ...  \n",
       "1622  Training Data Selection \\nfor Optimal Generalization \\nin Trigonometric Polynomial Networks \\nMasashi Sugiyama*and Hidemitsu Ogawa \\nDepartment of Computer Science, Tokyo Institute of Technology, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document']\n",
    "                 .isin([681, 9, 392, 1622, 17, \n",
    "                        906, 996, 503, 13, 733])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Research Papers per Topic based on Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>99.97</td>\n",
       "      <td>circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse</td>\n",
       "      <td>Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1103</td>\n",
       "      <td>2</td>\n",
       "      <td>99.97</td>\n",
       "      <td>neuron, cell, response, stimulus, activity, pattern, unit, layer, visual, synaptic, cortical, connection, firing, effect, et_al, neural, cortex, simulation, map, spike</td>\n",
       "      <td>Simulation of a Thalamocortical Circuit for \\nComputing Directional Heading in the Rat \\nHugh T. Blair* \\nDepartment of Psychology \\nYale University \\nNew Haven, CT 06520-8205 \\ntadb @minerva. cis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348</td>\n",
       "      <td>3</td>\n",
       "      <td>99.97</td>\n",
       "      <td>state, control, action, step, policy, trajectory, task, controller, reinforcement_learning, optimal, environment, robot, dynamic, goal, reward, position, change, agent, td, current</td>\n",
       "      <td>Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384</td>\n",
       "      <td>4</td>\n",
       "      <td>98.41</td>\n",
       "      <td>node, structure, cluster, tree, graph, clustering, variable, representation, map, vector, level, edge, local, constraint, rule, mapping, region, object, approximation, part</td>\n",
       "      <td>Distributed Pecursive Structure Processing \\nGraldine Legendre \\nDepartment of \\nLinguistics \\nYoshiro Miyata \\nOptoelectronic \\nComputing Systems Center \\nUniversity of Colorado \\nBoulder, CO 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>471</td>\n",
       "      <td>5</td>\n",
       "      <td>98.45</td>\n",
       "      <td>rate, variable, spike, sample, estimate, signal, feature, channel, average, component, probability, noise, missing, eeg, search, risk, step, density, classification, solution</td>\n",
       "      <td>Data Analysis using G/SPLINES \\nDavid Rogers \\nResearch Institute for Advanced Computer Science \\nMS T041-5, NASA/Ames Research Center \\nMoffett Field, CA 94035 \\nINTERNET: drogersriacs.edu \\nAbs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>416</td>\n",
       "      <td>6</td>\n",
       "      <td>99.97</td>\n",
       "      <td>unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level</td>\n",
       "      <td>A Recurrent Neural Network for Word Identification \\nfrom Continuous Phoneme Strings \\nRobert B. Allen \\nBellcore \\nMorristown, NJ 07962-1910 \\nCandace A. Kamm \\nBellcore \\nMorristown, NJ 07962-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>632</td>\n",
       "      <td>7</td>\n",
       "      <td>99.97</td>\n",
       "      <td>training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear</td>\n",
       "      <td>Computing with Almost Optimal Size Neural \\nNetworks \\nKai-Yeung Siu \\nDept. of Electrical &amp;; Comp. Engineering \\nUniversity of California, Irvine \\nIrvine, CA 92717 \\nVwani Roychowdhury \\nSchool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1645</td>\n",
       "      <td>8</td>\n",
       "      <td>98.78</td>\n",
       "      <td>image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape</td>\n",
       "      <td>Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1652</td>\n",
       "      <td>9</td>\n",
       "      <td>99.97</td>\n",
       "      <td>class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression</td>\n",
       "      <td>v-Arc: Ensemble Learning \\nin the Presence of Outliers \\nG. Ritsch t, B. Sch51kopf t, A. Smola*, \\nK.-R. Miillert, T. Onoda**, and S. Mika* \\nt GMD FIRST, Rudower Chaussee 5, 12489 Berlin, German...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1637</td>\n",
       "      <td>10</td>\n",
       "      <td>99.96</td>\n",
       "      <td>distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule</td>\n",
       "      <td>Dynamics of Supervised Learning with \\nRestricted Training Sets and Noisy Teachers \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nThe Strand, London WC2R 2LS, UK \\ntcoolen @mth.kc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Document  Dominant Topic  Contribution %  \\\n",
       "Dominant Topic                                             \n",
       "1                   1032               1           99.97   \n",
       "2                   1103               2           99.97   \n",
       "3                    348               3           99.97   \n",
       "4                    384               4           98.41   \n",
       "5                    471               5           98.45   \n",
       "6                    416               6           99.97   \n",
       "7                    632               7           99.97   \n",
       "8                   1645               8           98.78   \n",
       "9                   1652               9           99.97   \n",
       "10                  1637              10           99.96   \n",
       "\n",
       "                                                                                                                                                                                                   Topic Desc  \\\n",
       "Dominant Topic                                                                                                                                                                                                  \n",
       "1                            circuit, signal, neuron, chip, motion, current, voltage, analog, frequency, neural, filter, response, noise, channel, processing, cell, velocity, implementation, sound, synapse   \n",
       "2                                     neuron, cell, response, stimulus, activity, pattern, unit, layer, visual, synaptic, cortical, connection, firing, effect, et_al, neural, cortex, simulation, map, spike   \n",
       "3                        state, control, action, step, policy, trajectory, task, controller, reinforcement_learning, optimal, environment, robot, dynamic, goal, reward, position, change, agent, td, current   \n",
       "4                                node, structure, cluster, tree, graph, clustering, variable, representation, map, vector, level, edge, local, constraint, rule, mapping, region, object, approximation, part   \n",
       "5                              rate, variable, spike, sample, estimate, signal, feature, channel, average, component, probability, noise, missing, eeg, search, risk, step, density, classification, solution   \n",
       "6                     unit, training, pattern, word, task, rule, trained, recognition, layer, feature, sequence, architecture, representation, memory, net, hidden_unit, activation, speech, character, level   \n",
       "7                                 training, unit, vector, layer, net, state, hidden_unit, linear, neuron, architecture, recurrent, class, node, hmm, threshold, activation, gradient, mlp, pattern, nonlinear   \n",
       "8                      image, feature, object, pixel, face, vector, representation, transformation, recognition, view, local, visual, distance, class, linear, classification, scale, training, filter, shape   \n",
       "9               class, training, classifier, classification, probability, sample, distribution, kernel, bound, test, prediction, let, size, machine, estimate, loss, linear, training_set, vector, regression   \n",
       "10                         distribution, equation, state, matrix, vector, noise, dynamic, gaussian, approximation, solution, density, variable, probability, component, theory, linear, eq, step, prior, rule   \n",
       "\n",
       "                                                                                                                                                                                                                  Paper  \n",
       "Dominant Topic                                                                                                                                                                                                           \n",
       "1               Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...  \n",
       "2               Simulation of a Thalamocortical Circuit for \\nComputing Directional Heading in the Rat \\nHugh T. Blair* \\nDepartment of Psychology \\nYale University \\nNew Haven, CT 06520-8205 \\ntadb @minerva. cis...  \n",
       "3               Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...  \n",
       "4               Distributed Pecursive Structure Processing \\nGraldine Legendre \\nDepartment of \\nLinguistics \\nYoshiro Miyata \\nOptoelectronic \\nComputing Systems Center \\nUniversity of Colorado \\nBoulder, CO 80...  \n",
       "5               Data Analysis using G/SPLINES \\nDavid Rogers \\nResearch Institute for Advanced Computer Science \\nMS T041-5, NASA/Ames Research Center \\nMoffett Field, CA 94035 \\nINTERNET: drogersriacs.edu \\nAbs...  \n",
       "6               A Recurrent Neural Network for Word Identification \\nfrom Continuous Phoneme Strings \\nRobert B. Allen \\nBellcore \\nMorristown, NJ 07962-1910 \\nCandace A. Kamm \\nBellcore \\nMorristown, NJ 07962-19...  \n",
       "7               Computing with Almost Optimal Size Neural \\nNetworks \\nKai-Yeung Siu \\nDept. of Electrical &; Comp. Engineering \\nUniversity of California, Irvine \\nIrvine, CA 92717 \\nVwani Roychowdhury \\nSchool ...  \n",
       "8               Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...  \n",
       "9               v-Arc: Ensemble Learning \\nin the Presence of Outliers \\nG. Ritsch t, B. Sch51kopf t, A. Smola*, \\nK.-R. Miillert, T. Onoda**, and S. Mika* \\nt GMD FIRST, Rudower Chaussee 5, 12489 Berlin, German...  \n",
       "10              Dynamics of Supervised Learning with \\nRestricted Training Sets and Noisy Teachers \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nThe Strand, London WC2R 2LS, UK \\ntcoolen @mth.kc...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], \n",
    "                                                                                         ascending=False)\n",
    "                                                                             .iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Topics for New Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New Papers: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# papers manually downloaded from NIPS 16\n",
    "#https://papers.nips.cc/paper/2016\n",
    "\n",
    "new_paper_files = glob.glob('nips16*.txt')\n",
    "new_papers = []\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "              \n",
    "print('Total New Papers:', len(new_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(documents, normalizer_fn, bigram_model):\n",
    "    norm_docs = normalizer_fn(documents)\n",
    "    norm_docs_bigrams = bigram_model[norm_docs]\n",
    "    return norm_docs_bigrams\n",
    "\n",
    "def bow_features_pipeline(tokenized_docs, dictionary):\n",
    "    paper_bow_features = [dictionary.doc2bow(text) \n",
    "                              for text in tokenized_docs]\n",
    "    return paper_bow_features\n",
    "\n",
    "norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus, \n",
    "                                              bigram_model=bigram_model)\n",
    "norm_bow_features = bow_features_pipeline(tokenized_docs=norm_new_papers, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_new_papers[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_bow_features[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_predictions(topic_model, corpus, topn=3):\n",
    "    topic_predictions = topic_model[corpus]\n",
    "    best_topics = [[(topic, round(wt, 3)) \n",
    "                        for topic, wt in sorted(topic_predictions[i], \n",
    "                                                key=lambda row: -row[1])[:topn]] \n",
    "                            for i in range(len(topic_predictions))]\n",
    "    return best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_preds = get_topic_predictions(topic_model=best_lda_model,  corpus=norm_bow_features, topn=2)\n",
    "topic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, wt in item] for item in topic_preds]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Contribution %'] = [topic_wt for topic_list in \n",
    "                                        [[round(wt*100, 2) \n",
    "                                              for topic_num, wt in item] \n",
    "                                                 for item in topic_preds] \n",
    "                                    for topic_wt in topic_list]\n",
    "\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
