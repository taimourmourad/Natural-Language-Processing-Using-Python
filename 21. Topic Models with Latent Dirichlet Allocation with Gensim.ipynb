{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nips12raw_str602', <http.client.HTTPMessage at 0x1c411e8c6a0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf nips12raw_str602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx', 'MATLAB_NOTES', 'nips00', 'nips01', 'nips02', 'nips03', 'nips04', 'nips05', 'nips06', 'nips07', 'nips08', 'nips09', 'nips10', 'nips11', 'nips12', 'orig', 'RAW_DATA_NOTES', 'README_yann']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['nips{0:02}'.format(i) for i in range(0, 13)]\n",
    "# Read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:#seperate 'em with /\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, it looks like the OCR hasn’t worked perfectly and we have\n",
    "some missing characters here and there. This is expected, but also makes this task more\n",
    "challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a problem \n",
      "from examples using a local learning rule, we prove that the entropy of the \n",
      "problem becomes a lower bound for the connectivity of the network. \n",
      "INTRODUCTION \n",
      "The most distinguishing feature of neural networks is their ability to spon- \n",
      "taneously learn the desired function from 'training' samples, i.e., their ability \n",
      "to program themselves. Clearly, a given neural network cannot just learn any \n",
      "function, there must be some restrictions on which networks can learn which \n",
      "functions. One obv\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')#any word\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]# word tokenization\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu', 'mostafa', 'california', 'institute', 'technology', 'pasadena', 'ca', 'abstract', 'doe', 'connectivity', 'neural', 'network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean', 'function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using']\n"
     ]
    }
   ],
   "source": [
    "# Viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start building topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let’s get started by looking at ways to generate phrases with\n",
    "influential bi-grams and remove some terms that may not be useful before feature\n",
    "engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Featuer Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feature engineering and vectorization, we want to extract some useful bi-gram\n",
    "based phrases from our research papers and remove some unnecessary terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu_mostafa', 'california_institute', 'technology_pasadena', 'ca_abstract', 'doe', 'connectivity', 'neural_network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean_function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using', 'local', 'learning', 'rule', 'prove', 'entropy', 'problem']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_papers[0]][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s generate phrases for all our tokenized research papers and build a vocabulary\n",
    "that will help us obtain a unique term/phrase to number mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings:  [(0, '0a'), (1, '2h'), (2, '2h2'), (3, '2he'), (4, '2n'), (5, '__c'), (6, '_c'), (7, '_k'), (8, 'a2'), (9, 'ability'), (10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated')]\n",
      "Total Vocabulary Size:  78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# Create a dictionary representationi of the docuemnts:\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings: ', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a lot of unique phrases in our corpus of research papers,\n",
    "based on the preceding output. Several of these terms are not very useful since they are\n",
    "specific to a paper or even a paragraph in a research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is time to prune\n",
    "our vocabulary and start removing terms. Leveraging document frequency is a great way\n",
    "to achieve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size:  7756\n"
     ]
    }
   ],
   "source": [
    "# fitler out words that occur less than 20 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding\n",
    "different themes and topics and not recurring themes. Hence, this suits our scenario\n",
    "perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now perform feature engineering by leveraging a simple Bag of Words\n",
    "model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (12, 3), (14, 1), (15, 1), (16, 1), (17, 16), (20, 1), (24, 1), (26, 1), (31, 3), (35, 1), (36, 1), (40, 3), (41, 5), (42, 1), (48, 1), (53, 3), (55, 1), (56, 2), (58, 1), (60, 3), (63, 5), (64, 4), (65, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 3), (82, 1), (83, 4), (84, 1), (85, 1), (86, 2), (94, 1), (96, 2), (97, 3), (106, 1), (110, 1), (119, 2), (120, 4), (121, 2), (124, 2), (127, 1), (128, 1), (132, 1), (133, 1), (135, 6), (136, 1), (144, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ability', 1), ('aip', 3), ('although', 1), ('american_institute', 1), ('amount', 1), ('analog', 16), ('appears', 1), ('architecture', 1), ('aspect', 1), ('available', 3), ('become', 1), ('becomes', 1), ('binary', 3), ('biological', 5), ('bit', 1), ('cannot', 1), ('circuit', 3), ('collective', 1), ('compare', 2), ('complex', 1), ('computing', 3), ('conference', 5), ('connected', 4), ('connectivity', 2), ('define', 1), ('defined', 1), ('defines', 1), ('definition', 1), ('denker', 3), ('designed', 1), ('desired', 4), ('diagonal', 1), ('difference', 1), ('directly', 2), ('ed', 1), ('el', 2), ('element', 3), ('equivalent', 1), ('eventually', 1), ('feature', 2), ('final', 4), ('find', 2), ('fixed', 2), ('frequency', 1), ('furthermore', 1), ('generating', 1), ('get', 1), ('global', 6), ('go', 1), ('hence', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Viewing actual terms and their counts\n",
    "print([(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers:  1740\n"
     ]
    }
   ],
   "source": [
    "# total papers in the corpus\n",
    "print('Total number of papers: ', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our documents are now processed and have a good enough representation with the\n",
    "Bag of Words model to begin modeling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto', \n",
    "                                   random_state=42, iterations=500, num_topics=TOTAL_TOPICS, passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the topics in our trained topic model is quite easy and we can generate them\n",
    "with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.013*\"circuit\" + 0.012*\"chip\" + 0.008*\"neuron\" + 0.008*\"analog\" + 0.007*\"current\" + 0.007*\"bit\" + 0.006*\"voltage\" + 0.005*\"node\" + 0.005*\"word\" + 0.005*\"vector\" + 0.005*\"processor\" + 0.004*\"implementation\" + 0.004*\"threshold\" + 0.004*\"computation\" + 0.004*\"element\" + 0.004*\"signal\" + 0.004*\"pattern\" + 0.004*\"design\" + 0.004*\"memory\" + 0.004*\"parallel\"\n",
      "\n",
      "Topic #2:\n",
      "0.030*\"image\" + 0.012*\"object\" + 0.011*\"feature\" + 0.006*\"pixel\" + 0.006*\"visual\" + 0.005*\"representation\" + 0.005*\"recognition\" + 0.005*\"unit\" + 0.005*\"motion\" + 0.005*\"face\" + 0.005*\"task\" + 0.004*\"view\" + 0.004*\"layer\" + 0.004*\"human\" + 0.004*\"training\" + 0.004*\"position\" + 0.004*\"location\" + 0.004*\"region\" + 0.004*\"character\" + 0.003*\"vector\"\n",
      "\n",
      "Topic #3:\n",
      "0.020*\"neuron\" + 0.017*\"cell\" + 0.012*\"response\" + 0.010*\"stimulus\" + 0.007*\"spike\" + 0.007*\"signal\" + 0.006*\"activity\" + 0.006*\"synaptic\" + 0.005*\"firing\" + 0.005*\"frequency\" + 0.005*\"pattern\" + 0.004*\"current\" + 0.004*\"effect\" + 0.004*\"neural\" + 0.004*\"change\" + 0.004*\"et_al\" + 0.004*\"channel\" + 0.004*\"synapsis\" + 0.003*\"motion\" + 0.003*\"unit\"\n",
      "\n",
      "Topic #4:\n",
      "0.013*\"neuron\" + 0.009*\"cell\" + 0.009*\"pattern\" + 0.008*\"activity\" + 0.007*\"map\" + 0.006*\"unit\" + 0.006*\"dynamic\" + 0.006*\"visual\" + 0.005*\"layer\" + 0.005*\"receptive_field\" + 0.005*\"orientation\" + 0.005*\"connection\" + 0.005*\"cortical\" + 0.005*\"correlation\" + 0.004*\"response\" + 0.004*\"feature\" + 0.004*\"cortex\" + 0.004*\"stimulus\" + 0.004*\"phase\" + 0.004*\"neural\"\n",
      "\n",
      "Topic #5:\n",
      "0.020*\"unit\" + 0.011*\"state\" + 0.010*\"training\" + 0.008*\"rule\" + 0.007*\"net\" + 0.006*\"word\" + 0.006*\"pattern\" + 0.006*\"sequence\" + 0.006*\"node\" + 0.006*\"layer\" + 0.006*\"hidden_unit\" + 0.005*\"activation\" + 0.005*\"architecture\" + 0.005*\"recurrent\" + 0.004*\"recognition\" + 0.004*\"task\" + 0.004*\"vector\" + 0.004*\"trained\" + 0.004*\"context\" + 0.004*\"connection\"\n",
      "\n",
      "Topic #6:\n",
      "0.017*\"signal\" + 0.013*\"memory\" + 0.010*\"noise\" + 0.009*\"control\" + 0.008*\"trajectory\" + 0.007*\"dynamic\" + 0.007*\"state\" + 0.006*\"movement\" + 0.005*\"motor\" + 0.005*\"mapping\" + 0.004*\"pattern\" + 0.004*\"feedback\" + 0.004*\"capacity\" + 0.004*\"position\" + 0.004*\"speech\" + 0.004*\"training\" + 0.004*\"target\" + 0.004*\"arm\" + 0.004*\"vector\" + 0.004*\"change\"\n",
      "\n",
      "Topic #7:\n",
      "0.007*\"vector\" + 0.006*\"equation\" + 0.005*\"let\" + 0.005*\"linear\" + 0.005*\"distribution\" + 0.005*\"approximation\" + 0.005*\"matrix\" + 0.004*\"theorem\" + 0.004*\"convergence\" + 0.004*\"bound\" + 0.004*\"class\" + 0.004*\"training\" + 0.004*\"optimal\" + 0.004*\"theory\" + 0.004*\"consider\" + 0.004*\"solution\" + 0.004*\"probability\" + 0.004*\"estimate\" + 0.004*\"noise\" + 0.003*\"rate\"\n",
      "\n",
      "Topic #8:\n",
      "0.010*\"distribution\" + 0.009*\"probability\" + 0.009*\"variable\" + 0.008*\"mixture\" + 0.006*\"gaussian\" + 0.006*\"tree\" + 0.006*\"prior\" + 0.006*\"structure\" + 0.006*\"component\" + 0.006*\"node\" + 0.005*\"density\" + 0.005*\"class\" + 0.004*\"likelihood\" + 0.004*\"bayesian\" + 0.004*\"estimate\" + 0.004*\"sample\" + 0.004*\"step\" + 0.004*\"log\" + 0.004*\"source\" + 0.003*\"approximation\"\n",
      "\n",
      "Topic #9:\n",
      "0.017*\"training\" + 0.010*\"classifier\" + 0.008*\"classification\" + 0.008*\"class\" + 0.006*\"pattern\" + 0.006*\"feature\" + 0.006*\"test\" + 0.006*\"training_set\" + 0.005*\"vector\" + 0.005*\"prediction\" + 0.004*\"kernel\" + 0.004*\"experiment\" + 0.004*\"trained\" + 0.004*\"linear\" + 0.003*\"technique\" + 0.003*\"rbf\" + 0.003*\"task\" + 0.003*\"size\" + 0.003*\"table\" + 0.003*\"sample\"\n",
      "\n",
      "Topic #10:\n",
      "0.026*\"state\" + 0.013*\"action\" + 0.012*\"control\" + 0.008*\"policy\" + 0.007*\"task\" + 0.007*\"step\" + 0.006*\"reinforcement_learning\" + 0.006*\"controller\" + 0.006*\"environment\" + 0.005*\"optimal\" + 0.005*\"robot\" + 0.004*\"goal\" + 0.004*\"reward\" + 0.003*\"agent\" + 0.003*\"td\" + 0.003*\"current\" + 0.003*\"trial\" + 0.003*\"cost\" + 0.003*\"rate\" + 0.003*\"reinforcement\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can also view the overall mean coherence score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -0.9858031202745918\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s\n",
    "now look at the output of our LDA topic model in an easier to understand format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One\n",
    "way is to visualize the topics as tuples of terms and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('vector', 0.007), ('equation', 0.006), ('let', 0.005), ('linear', 0.005), ('distribution', 0.005), ('approximation', 0.005), ('matrix', 0.005), ('theorem', 0.004), ('convergence', 0.004), ('bound', 0.004), ('class', 0.004), ('training', 0.004), ('optimal', 0.004), ('theory', 0.004), ('consider', 0.004), ('solution', 0.004), ('probability', 0.004), ('estimate', 0.004), ('noise', 0.004), ('rate', 0.003)]\n",
      "\n",
      "Topic #2:\n",
      "[('training', 0.017), ('classifier', 0.01), ('classification', 0.008), ('class', 0.008), ('pattern', 0.006), ('feature', 0.006), ('test', 0.006), ('training_set', 0.006), ('vector', 0.005), ('prediction', 0.005), ('kernel', 0.004), ('experiment', 0.004), ('trained', 0.004), ('linear', 0.004), ('technique', 0.003), ('rbf', 0.003), ('task', 0.003), ('size', 0.003), ('table', 0.003), ('sample', 0.003)]\n",
      "\n",
      "Topic #3:\n",
      "[('neuron', 0.02), ('cell', 0.017), ('response', 0.012), ('stimulus', 0.01), ('spike', 0.007), ('signal', 0.007), ('activity', 0.006), ('synaptic', 0.006), ('firing', 0.005), ('frequency', 0.005), ('pattern', 0.005), ('current', 0.004), ('effect', 0.004), ('neural', 0.004), ('change', 0.004), ('et_al', 0.004), ('channel', 0.004), ('synapsis', 0.004), ('motion', 0.003), ('unit', 0.003)]\n",
      "\n",
      "Topic #4:\n",
      "[('unit', 0.02), ('state', 0.011), ('training', 0.01), ('rule', 0.008), ('net', 0.007), ('word', 0.006), ('pattern', 0.006), ('sequence', 0.006), ('node', 0.006), ('layer', 0.006), ('hidden_unit', 0.006), ('activation', 0.005), ('architecture', 0.005), ('recurrent', 0.005), ('recognition', 0.004), ('task', 0.004), ('vector', 0.004), ('trained', 0.004), ('context', 0.004), ('connection', 0.004)]\n",
      "\n",
      "Topic #5:\n",
      "[('circuit', 0.013), ('chip', 0.012), ('neuron', 0.008), ('analog', 0.008), ('current', 0.007), ('bit', 0.007), ('voltage', 0.006), ('node', 0.005), ('word', 0.005), ('vector', 0.005), ('processor', 0.005), ('implementation', 0.004), ('threshold', 0.004), ('computation', 0.004), ('element', 0.004), ('signal', 0.004), ('pattern', 0.004), ('design', 0.004), ('memory', 0.004), ('parallel', 0.004)]\n",
      "\n",
      "Topic #6:\n",
      "[('neuron', 0.013), ('cell', 0.009), ('pattern', 0.009), ('activity', 0.008), ('map', 0.007), ('unit', 0.006), ('dynamic', 0.006), ('visual', 0.006), ('layer', 0.005), ('receptive_field', 0.005), ('orientation', 0.005), ('connection', 0.005), ('cortical', 0.005), ('correlation', 0.005), ('response', 0.004), ('feature', 0.004), ('cortex', 0.004), ('stimulus', 0.004), ('phase', 0.004), ('neural', 0.004)]\n",
      "\n",
      "Topic #7:\n",
      "[('image', 0.03), ('object', 0.012), ('feature', 0.011), ('pixel', 0.006), ('visual', 0.006), ('representation', 0.005), ('recognition', 0.005), ('unit', 0.005), ('motion', 0.005), ('face', 0.005), ('task', 0.005), ('view', 0.004), ('layer', 0.004), ('human', 0.004), ('training', 0.004), ('position', 0.004), ('location', 0.004), ('region', 0.004), ('character', 0.004), ('vector', 0.003)]\n",
      "\n",
      "Topic #8:\n",
      "[('distribution', 0.01), ('probability', 0.009), ('variable', 0.009), ('mixture', 0.008), ('gaussian', 0.006), ('tree', 0.006), ('prior', 0.006), ('structure', 0.006), ('component', 0.006), ('node', 0.006), ('density', 0.005), ('class', 0.005), ('likelihood', 0.004), ('bayesian', 0.004), ('estimate', 0.004), ('sample', 0.004), ('step', 0.004), ('log', 0.004), ('source', 0.004), ('approximation', 0.003)]\n",
      "\n",
      "Topic #9:\n",
      "[('signal', 0.017), ('memory', 0.013), ('noise', 0.01), ('control', 0.009), ('trajectory', 0.008), ('dynamic', 0.007), ('state', 0.007), ('movement', 0.006), ('motor', 0.005), ('mapping', 0.005), ('pattern', 0.004), ('feedback', 0.004), ('capacity', 0.004), ('position', 0.004), ('speech', 0.004), ('training', 0.004), ('target', 0.004), ('arm', 0.004), ('vector', 0.004), ('change', 0.004)]\n",
      "\n",
      "Topic #10:\n",
      "[('state', 0.026), ('action', 0.013), ('control', 0.012), ('policy', 0.008), ('task', 0.007), ('step', 0.007), ('reinforcement_learning', 0.006), ('controller', 0.006), ('environment', 0.006), ('optimal', 0.005), ('robot', 0.005), ('goal', 0.004), ('reward', 0.004), ('agent', 0.003), ('td', 0.003), ('current', 0.003), ('trial', 0.003), ('cost', 0.003), ('rate', 0.003), ('reinforcement', 0.003)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the topics as a list of terms without the weights when we want to\n",
    "understand the context or theme conveyed by each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['vector', 'equation', 'let', 'linear', 'distribution', 'approximation', 'matrix', 'theorem', 'convergence', 'bound', 'class', 'training', 'optimal', 'theory', 'consider', 'solution', 'probability', 'estimate', 'noise', 'rate']\n",
      "\n",
      "Topic #2:\n",
      "['training', 'classifier', 'classification', 'class', 'pattern', 'feature', 'test', 'training_set', 'vector', 'prediction', 'kernel', 'experiment', 'trained', 'linear', 'technique', 'rbf', 'task', 'size', 'table', 'sample']\n",
      "\n",
      "Topic #3:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'spike', 'signal', 'activity', 'synaptic', 'firing', 'frequency', 'pattern', 'current', 'effect', 'neural', 'change', 'et_al', 'channel', 'synapsis', 'motion', 'unit']\n",
      "\n",
      "Topic #4:\n",
      "['unit', 'state', 'training', 'rule', 'net', 'word', 'pattern', 'sequence', 'node', 'layer', 'hidden_unit', 'activation', 'architecture', 'recurrent', 'recognition', 'task', 'vector', 'trained', 'context', 'connection']\n",
      "\n",
      "Topic #5:\n",
      "['circuit', 'chip', 'neuron', 'analog', 'current', 'bit', 'voltage', 'node', 'word', 'vector', 'processor', 'implementation', 'threshold', 'computation', 'element', 'signal', 'pattern', 'design', 'memory', 'parallel']\n",
      "\n",
      "Topic #6:\n",
      "['neuron', 'cell', 'pattern', 'activity', 'map', 'unit', 'dynamic', 'visual', 'layer', 'receptive_field', 'orientation', 'connection', 'cortical', 'correlation', 'response', 'feature', 'cortex', 'stimulus', 'phase', 'neural']\n",
      "\n",
      "Topic #7:\n",
      "['image', 'object', 'feature', 'pixel', 'visual', 'representation', 'recognition', 'unit', 'motion', 'face', 'task', 'view', 'layer', 'human', 'training', 'position', 'location', 'region', 'character', 'vector']\n",
      "\n",
      "Topic #8:\n",
      "['distribution', 'probability', 'variable', 'mixture', 'gaussian', 'tree', 'prior', 'structure', 'component', 'node', 'density', 'class', 'likelihood', 'bayesian', 'estimate', 'sample', 'step', 'log', 'source', 'approximation']\n",
      "\n",
      "Topic #9:\n",
      "['signal', 'memory', 'noise', 'control', 'trajectory', 'dynamic', 'state', 'movement', 'motor', 'mapping', 'pattern', 'feedback', 'capacity', 'position', 'speech', 'training', 'target', 'arm', 'vector', 'change']\n",
      "\n",
      "Topic #10:\n",
      "['state', 'action', 'control', 'policy', 'task', 'step', 'reinforcement_learning', 'controller', 'environment', 'optimal', 'robot', 'goal', 'reward', 'agent', 'td', 'current', 'trial', 'cost', 'rate', 'reinforcement']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.4930044902277785\n",
      "Avg. Coherence Score (UMass): -0.9858031202745918\n",
      "Model Perplexity: -7.787864245063152\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary, \n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary, \n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
