{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nips12raw_str602', <http.client.HTTPMessage at 0x7f86860fb490>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf nips12raw_str602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orig', 'nips06', 'nips08', 'nips00', 'nips04', 'idx', 'nips02', 'README_yann', 'nips10', 'nips11', 'nips01', 'nips03', 'nips09', 'nips12', 'nips07', 'MATLAB_NOTES', 'nips05', 'RAW_DATA_NOTES']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['nips{0:02}'.format(i) for i in range(0, 13)]\n",
    "# Read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:#seperate 'em with /\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, it looks like the OCR hasn’t worked perfectly and we have\n",
    "some missing characters here and there. This is expected, but also makes this task more\n",
    "challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804 \n",
      "INTRODUCTION TO A SYSTEM FOR IMPLEMENTING NEURAL NET \n",
      "CONNECTIONS ON SIMD ARCHITECTURES \n",
      "Sherryl Tomboulian \n",
      "Institute for Computer Applications in Science and Engineering \n",
      "NASA Langley Research Center, Hampton VA 23665 \n",
      "ABSTRACT \n",
      "Neural networks have attracted much interest recently, and using parallel \n",
      "architectures to simulate neural networks is a natural and necessary applica- \n",
      "tion. The SIMD model of parallel computation is chosen, because systems of \n",
      "this type can be built with large numbers of processing elements. However, \n",
      "such systems are not naturally suited to generalized communication. A method \n",
      "is proposed that allows an implementation of neural network connections on \n",
      "massively parallel SIMD architectures. The key to this system is an algorithm \n",
      "that allows the formation of arbitrary connections between the 'neurons '. A \n",
      "feature is the ability to add new connections quickly. It also has error recov- \n",
      "ery ability and is robust over a variety of network topologies. Si\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]# word tokenization\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'system', 'implementing', 'neural', 'net', 'connection', 'simd', 'architecture', 'sherryl', 'tomboulian', 'institute', 'computer', 'application', 'science', 'engineering', 'nasa', 'langley', 'research', 'center', 'hampton', 'va', 'abstract', 'neural', 'network', 'attracted', 'much', 'interest', 'recently', 'using', 'parallel', 'architecture', 'simulate', 'neural', 'network', 'natural', 'necessary', 'applica', 'tion', 'simd', 'model', 'parallel', 'computation', 'chosen', 'system', 'type', 'built', 'large', 'number', 'processing', 'element']\n"
     ]
    }
   ],
   "source": [
    "# Viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we present out text data in thr form of a Bag of Words model with uni-gram and bi-gram, similar to our analyses in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 14408)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(min_df=20, max_df=0.6,ngram_range=(1, 2),token_pattern=None,tokenizer=lambda doc:doc,preprocessor=lambda doc:doc)\n",
    "cv_features = cv.fit_transform(norm_papers)\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 14408\n"
     ]
    }
   ],
   "source": [
    "# validating vocaublary size\n",
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50, learning_method='online'\n",
    "                                      , batch_size=1740, learning_offset=50., random_state=42, n_jobs=16)\n",
    "document_topics = lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then obtain the topic-term matrix and build a dataframe from it to showcase\n",
    "the topics and terms in an easy-to-interpret format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = 20\n",
    "topic_terms = lda_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,columns = ['Terms per Topic'],index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics,  columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see some repetition in similar\n",
    "themes among the topics, which might be an indication that this model is not as good\n",
    "as our MALLET LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the research papers having the maximum\n",
    "contribution of each of the 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_contrib_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_contrib_topics.index\n",
    "contrib_perc = max_contrib_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0] for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can see that some topics have a very\n",
    "poor representation of almost 0% in the corpus and so we see the same paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics with a good\n",
    "contribution (almost 100% dominance) showcase papers that are closely correlated\n",
    "with the theme conveyed by the corresponding topic, including reinforcement learning,\n",
    "Bayesian and Gaussian mixture models, neural models on VLSI, and transistors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
