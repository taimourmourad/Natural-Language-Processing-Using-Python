{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nips12raw_str602', <http.client.HTTPMessage at 0x7f86860fb490>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf nips12raw_str602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orig', 'nips06', 'nips08', 'nips00', 'nips04', 'idx', 'nips02', 'README_yann', 'nips10', 'nips11', 'nips01', 'nips03', 'nips09', 'nips12', 'nips07', 'MATLAB_NOTES', 'nips05', 'RAW_DATA_NOTES']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['nips{0:02}'.format(i) for i in range(0, 13)]\n",
    "# Read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:#seperate 'em with /\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, it looks like the OCR hasn’t worked perfectly and we have\n",
    "some missing characters here and there. This is expected, but also makes this task more\n",
    "challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804 \n",
      "INTRODUCTION TO A SYSTEM FOR IMPLEMENTING NEURAL NET \n",
      "CONNECTIONS ON SIMD ARCHITECTURES \n",
      "Sherryl Tomboulian \n",
      "Institute for Computer Applications in Science and Engineering \n",
      "NASA Langley Research Center, Hampton VA 23665 \n",
      "ABSTRACT \n",
      "Neural networks have attracted much interest recently, and using parallel \n",
      "architectures to simulate neural networks is a natural and necessary applica- \n",
      "tion. The SIMD model of parallel computation is chosen, because systems of \n",
      "this type can be built with large numbers of processing elements. However, \n",
      "such systems are not naturally suited to generalized communication. A method \n",
      "is proposed that allows an implementation of neural network connections on \n",
      "massively parallel SIMD architectures. The key to this system is an algorithm \n",
      "that allows the formation of arbitrary connections between the 'neurons '. A \n",
      "feature is the ability to add new connections quickly. It also has error recov- \n",
      "ery ability and is robust over a variety of network topologies. Si\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]# word tokenization\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'system', 'implementing', 'neural', 'net', 'connection', 'simd', 'architecture', 'sherryl', 'tomboulian', 'institute', 'computer', 'application', 'science', 'engineering', 'nasa', 'langley', 'research', 'center', 'hampton', 'va', 'abstract', 'neural', 'network', 'attracted', 'much', 'interest', 'recently', 'using', 'parallel', 'architecture', 'simulate', 'neural', 'network', 'natural', 'necessary', 'applica', 'tion', 'simd', 'model', 'parallel', 'computation', 'chosen', 'system', 'type', 'built', 'large', 'number', 'processing', 'element']\n"
     ]
    }
   ],
   "source": [
    "# Viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we present out text data in thr form of a Bag of Words model with uni-gram and bi-gram, similar to our analyses in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 14408)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(min_df=20, max_df=0.6,ngram_range=(1, 2),token_pattern=None,tokenizer=lambda doc:doc,preprocessor=lambda doc:doc)\n",
    "cv_features = cv.fit_transform(norm_papers)\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 14408\n"
     ]
    }
   ],
   "source": [
    "# validating vocaublary size\n",
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50, learning_method='online'\n",
    "                                      , batch_size=1740, learning_offset=50., random_state=42, n_jobs=16)\n",
    "document_topics = lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then obtain the topic-term matrix and build a dataframe from it to showcase\n",
    "the topics and terms in an easy-to-interpret format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-3c498183ce06>:6: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>neuron, circuit, chip, analog, current, signal, voltage, channel, vlsi, implementation, bit, noise, pulse, processor, design, synapse, parallel, fig, line, digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, motion, recognition, node, net, matrix, object, gaussian, sequence, line, size, hidden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>motor, frequency, auditory, sound, template, command, spectrum, acoustic, syllable, control, feedback, amplitude, motor command, song, production, representation, onset, harmonic, temporal, phase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>cell, neuron, response, visual, stimulus, activity, spike, field, synaptic, motion, direction, firing, cortex, signal, orientation, spatial, eye, rate, map, fig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>image, feature, recognition, layer, hidden, task, object, speech, representation, trained, test, classification, net, classifier, level, architecture, class, experiment, rule, node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>state, dynamic, matrix, equation, rule, recurrent, gradient, fixed, neuron, solution, node, signal, hidden, sequence, net, activation, attractor, source, step, connection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>sequence, chain, region, structure, protein, prediction, hmms, site, receptor, gene, class, human, positive, distance, length, mouse, negative, cell, domain, sheet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>memory, word, context, similarity, item, recall, phoneme, activation, probability, representation, short, list, association, address, short term, state, serial, store, storage, phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>ii, activation, winner, region, take, winner take, texture, ii ii, behavior, self, saliency, competitive, wta, connection, binding, iii, edge, feedback, search, sensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, layer, rate, architecture, random, hidden, test, image, fig, change, field, generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>generalization, face, hidden unit, hidden, capacity, pca, image, teacher, student, principal, generalization error, committee, component, principal component, correlation, limit, facial, training set, expression, phase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>feature, image, distribution, neuron, state, class, hidden, probability, layer, node, equation, size, rate, matrix, line, prediction, signal, et, noise, variable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>image, state, cell, object, rule, layer, et al, neuron, distribution, step, visual, signal, field, dynamic, feature, probability, map, et, solution, matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>neuron, map, state, cell, rate, hidden, probability, field, equation, representation, node, signal, dynamic, layer, et al, sequence, test, prediction, recognition, noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>distribution, probability, variable, gaussian, class, approximation, estimate, sample, density, noise, mixture, matrix, log, likelihood, prior, optimal, xi, variance, bayesian, prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>state, neuron, rule, probability, layer, rate, memory, image, distribution, equation, signal, solution, response, class, theory, et, step, feature, variable, high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>state, feature, probability, image, layer, cell, field, neuron, task, rate, recognition, dynamic, variable, distribution, rule, representation, control, equation, net, class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>state, control, action, policy, reinforcement, optimal, task, step, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>state, control, feature, probability, neuron, hidden, architecture, task, rate, level, estimate, local, et, distribution, component, net, response, signal, image, optimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>threshold, theorem, bound, net, proof, size, polynomial, layer, let, depth, gate, neural net, bit, bounded, binary, constant, circuit, every, assume, integer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                    Terms per Topic\n",
       "Topic1   neuron, circuit, chip, analog, current, signal, voltage, channel, vlsi, implementation, bit, noise, pulse, processor, design, synapse, parallel, fig, line, digital                                                       \n",
       "Topic2   image, feature, structure, state, layer, neuron, distribution, local, cell, motion, recognition, node, net, matrix, object, gaussian, sequence, line, size, hidden                                                        \n",
       "Topic3   motor, frequency, auditory, sound, template, command, spectrum, acoustic, syllable, control, feedback, amplitude, motor command, song, production, representation, onset, harmonic, temporal, phase                       \n",
       "Topic4   cell, neuron, response, visual, stimulus, activity, spike, field, synaptic, motion, direction, firing, cortex, signal, orientation, spatial, eye, rate, map, fig                                                          \n",
       "Topic5   image, feature, recognition, layer, hidden, task, object, speech, representation, trained, test, classification, net, classifier, level, architecture, class, experiment, rule, node                                      \n",
       "Topic6   state, dynamic, matrix, equation, rule, recurrent, gradient, fixed, neuron, solution, node, signal, hidden, sequence, net, activation, attractor, source, step, connection                                                \n",
       "Topic7   sequence, chain, region, structure, protein, prediction, hmms, site, receptor, gene, class, human, positive, distance, length, mouse, negative, cell, domain, sheet                                                       \n",
       "Topic8   memory, word, context, similarity, item, recall, phoneme, activation, probability, representation, short, list, association, address, short term, state, serial, store, storage, phone                                    \n",
       "Topic9   ii, activation, winner, region, take, winner take, texture, ii ii, behavior, self, saliency, competitive, wta, connection, binding, iii, edge, feedback, search, sensor                                                   \n",
       "Topic10  state, cell, distribution, neuron, probability, control, response, signal, task, layer, rate, architecture, random, hidden, test, image, fig, change, field, generalization                                               \n",
       "Topic11  generalization, face, hidden unit, hidden, capacity, pca, image, teacher, student, principal, generalization error, committee, component, principal component, correlation, limit, facial, training set, expression, phase\n",
       "Topic12  feature, image, distribution, neuron, state, class, hidden, probability, layer, node, equation, size, rate, matrix, line, prediction, signal, et, noise, variable                                                         \n",
       "Topic13  image, state, cell, object, rule, layer, et al, neuron, distribution, step, visual, signal, field, dynamic, feature, probability, map, et, solution, matrix                                                               \n",
       "Topic14  neuron, map, state, cell, rate, hidden, probability, field, equation, representation, node, signal, dynamic, layer, et al, sequence, test, prediction, recognition, noise                                                 \n",
       "Topic15  distribution, probability, variable, gaussian, class, approximation, estimate, sample, density, noise, mixture, matrix, log, likelihood, prior, optimal, xi, variance, bayesian, prediction                               \n",
       "Topic16  state, neuron, rule, probability, layer, rate, memory, image, distribution, equation, signal, solution, response, class, theory, et, step, feature, variable, high                                                        \n",
       "Topic17  state, feature, probability, image, layer, cell, field, neuron, task, rate, recognition, dynamic, variable, distribution, rule, representation, control, equation, net, class                                             \n",
       "Topic18  state, control, action, policy, reinforcement, optimal, task, step, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm                        \n",
       "Topic19  state, control, feature, probability, neuron, hidden, architecture, task, rate, level, estimate, local, et, distribution, component, net, response, signal, image, optimal                                                \n",
       "Topic20  threshold, theorem, bound, net, proof, size, polynomial, layer, let, depth, gate, neural net, bit, bounded, binary, constant, circuit, every, assume, integer                                                             "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms = 20\n",
    "topic_terms = lda_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,columns = ['Terms per Topic'],index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1730</th>\n",
       "      <th>1731</th>\n",
       "      <th>1732</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1735</th>\n",
       "      <th>1736</th>\n",
       "      <th>1737</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.580</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>0.164</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T8</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T9</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T16</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T17</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T18</th>\n",
       "      <td>0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T19</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T20</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 1740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9  ...  1730  \\\n",
       "T1  0.580 0.305 0.134 0.016 0.000 0.642 0.239 0.003 0.444 0.172  ... 0.601   \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.000  ... 0.026   \n",
       "T4  0.000 0.000 0.079 0.002 0.000 0.079 0.000 0.013 0.232 0.210  ... 0.315   \n",
       "T5  0.054 0.032 0.068 0.444 0.380 0.027 0.129 0.546 0.282 0.355  ... 0.014   \n",
       "T6  0.164 0.511 0.448 0.227 0.410 0.114 0.422 0.138 0.000 0.067  ... 0.000   \n",
       "T7  0.000 0.020 0.000 0.000 0.000 0.000 0.000 0.000 0.011 0.000  ... 0.000   \n",
       "T8  0.005 0.005 0.000 0.000 0.000 0.000 0.072 0.000 0.008 0.000  ... 0.000   \n",
       "T9  0.006 0.000 0.000 0.000 0.004 0.000 0.053 0.000 0.000 0.000  ... 0.025   \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T11 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.005 0.000 0.000  ... 0.000   \n",
       "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T15 0.000 0.118 0.211 0.304 0.196 0.134 0.065 0.295 0.000 0.189  ... 0.000   \n",
       "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T18 0.182 0.000 0.052 0.000 0.000 0.000 0.000 0.000 0.000 0.006  ... 0.018   \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T20 0.009 0.007 0.007 0.006 0.009 0.004 0.005 0.000 0.000 0.000  ... 0.000   \n",
       "\n",
       "     1731  1732  1733  1734  1735  1736  1737  1738  1739  \n",
       "T1  0.006 0.000 0.000 0.000 0.006 0.000 0.000 0.000 0.000  \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.017 0.000  \n",
       "T4  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.043 0.000  \n",
       "T5  0.374 0.000 0.199 0.453 0.038 0.025 0.224 0.334 0.029  \n",
       "T6  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.136 0.010  \n",
       "T7  0.000 0.000 0.000 0.090 0.000 0.000 0.000 0.055 0.000  \n",
       "T8  0.000 0.004 0.000 0.006 0.000 0.000 0.006 0.000 0.000  \n",
       "T9  0.000 0.000 0.011 0.064 0.000 0.000 0.000 0.004 0.000  \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T11 0.000 0.010 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T15 0.229 0.080 0.606 0.357 0.923 0.941 0.769 0.196 0.173  \n",
       "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T18 0.381 0.905 0.162 0.029 0.029 0.034 0.000 0.216 0.788  \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T20 0.010 0.000 0.022 0.000 0.004 0.000 0.000 0.000 0.000  \n",
       "\n",
       "[20 rows x 1740 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics,  columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see some repetition in similar\n",
    "themes among the topics, which might be an indication that this model is not as good\n",
    "as our MALLET LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the research papers having the maximum\n",
    "contribution of each of the 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Paper Num</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Paper Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>T1</td>\n",
       "      <td>0.99930</td>\n",
       "      <td>942</td>\n",
       "      <td>neuron, circuit, chip, analog, current, signal, voltage, channel, vlsi, implementation, bit, noise, pulse, processor, design, synapse, parallel, fig, line, digital</td>\n",
       "      <td>Single Transistor Learning Synapses \\nPaul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\n(SlS) 95- 2S12 \\npaul@hobiecat.pcmp.calt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>T2</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, motion, recognition, node, net, matrix, object, gaussian, sequence, line, size, hidden</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>T3</td>\n",
       "      <td>0.76997</td>\n",
       "      <td>149</td>\n",
       "      <td>motor, frequency, auditory, sound, template, command, spectrum, acoustic, syllable, control, feedback, amplitude, motor command, song, production, representation, onset, harmonic, temporal, phase</td>\n",
       "      <td>795 \\nSONG LEARNING IN BIRDS \\nM. Konishi \\nDivision of Biology \\nCalifornia Institute of Technology \\nABSTRACT\\nBirds sing to communicate. Male birds use song to advertise their territories and \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>T4</td>\n",
       "      <td>0.99945</td>\n",
       "      <td>1103</td>\n",
       "      <td>cell, neuron, response, visual, stimulus, activity, spike, field, synaptic, motion, direction, firing, cortex, signal, orientation, spatial, eye, rate, map, fig</td>\n",
       "      <td>Simulation of a Thalamocortical Circuit for \\nComputing Directional Heading in the Rat \\nHugh T. Blair* \\nDepartment of Psychology \\nYale University \\nNew Haven, CT 06520-8205 \\ntadb @minerva. cis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>T5</td>\n",
       "      <td>0.99949</td>\n",
       "      <td>213</td>\n",
       "      <td>image, feature, recognition, layer, hidden, task, object, speech, representation, trained, test, classification, net, classifier, level, architecture, class, experiment, rule, node</td>\n",
       "      <td>266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>T6</td>\n",
       "      <td>0.98993</td>\n",
       "      <td>1084</td>\n",
       "      <td>state, dynamic, matrix, equation, rule, recurrent, gradient, fixed, neuron, solution, node, signal, hidden, sequence, net, activation, attractor, source, step, connection</td>\n",
       "      <td>Harmony Networks Do Not Work \\nRen5 Gourley \\nSchool of Computing Science \\nSimon Fraser University \\nBurnaby, B.C., V5A 1S6, Canada \\ngourley@mprgate.mpr.ca \\nAbstract \\nHarmony networks have be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>T7</td>\n",
       "      <td>0.99956</td>\n",
       "      <td>266</td>\n",
       "      <td>sequence, chain, region, structure, protein, prediction, hmms, site, receptor, gene, class, human, positive, distance, length, mouse, negative, cell, domain, sheet</td>\n",
       "      <td>A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>T8</td>\n",
       "      <td>0.97427</td>\n",
       "      <td>889</td>\n",
       "      <td>memory, word, context, similarity, item, recall, phoneme, activation, probability, representation, short, list, association, address, short term, state, serial, store, storage, phone</td>\n",
       "      <td>A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>T9</td>\n",
       "      <td>0.99929</td>\n",
       "      <td>281</td>\n",
       "      <td>ii, activation, winner, region, take, winner take, texture, ii ii, behavior, self, saliency, competitive, wta, connection, binding, iii, edge, feedback, search, sensor</td>\n",
       "      <td>44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>T10</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, layer, rate, architecture, random, hidden, test, image, fig, change, field, generalization</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>T11</td>\n",
       "      <td>0.99917</td>\n",
       "      <td>1289</td>\n",
       "      <td>generalization, face, hidden unit, hidden, capacity, pca, image, teacher, student, principal, generalization error, committee, component, principal component, correlation, limit, facial, training ...</td>\n",
       "      <td>I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>T12</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>feature, image, distribution, neuron, state, class, hidden, probability, layer, node, equation, size, rate, matrix, line, prediction, signal, et, noise, variable</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>T13</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>image, state, cell, object, rule, layer, et al, neuron, distribution, step, visual, signal, field, dynamic, feature, probability, map, et, solution, matrix</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>T14</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>neuron, map, state, cell, rate, hidden, probability, field, equation, representation, node, signal, dynamic, layer, et al, sequence, test, prediction, recognition, noise</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>T15</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>1691</td>\n",
       "      <td>distribution, probability, variable, gaussian, class, approximation, estimate, sample, density, noise, mixture, matrix, log, likelihood, prior, optimal, xi, variance, bayesian, prediction</td>\n",
       "      <td>Optimal Kernel Shapes for Local Linear \\nRegression \\nDirk Ormoneit Trevor Hastie \\nDepartment of Statistics \\nStanford University \\nStanford, CA 94305-4065 \\normoneit@stat. stanford. edu \\nAbstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>T16</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>state, neuron, rule, probability, layer, rate, memory, image, distribution, equation, signal, solution, response, class, theory, et, step, feature, variable, high</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>T17</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>state, feature, probability, image, layer, cell, field, neuron, task, rate, recognition, dynamic, variable, distribution, rule, representation, control, equation, net, class</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>T18</td>\n",
       "      <td>0.99942</td>\n",
       "      <td>348</td>\n",
       "      <td>state, control, action, policy, reinforcement, optimal, task, step, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
       "      <td>Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>T19</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>108</td>\n",
       "      <td>state, control, feature, probability, neuron, hidden, architecture, task, rate, level, estimate, local, et, distribution, component, net, response, signal, image, optimal</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>T20</td>\n",
       "      <td>0.68094</td>\n",
       "      <td>199</td>\n",
       "      <td>threshold, theorem, bound, net, proof, size, polynomial, layer, let, depth, gate, neural net, bit, bounded, binary, constant, circuit, every, assume, integer</td>\n",
       "      <td>702 Obradovic and Parberry \\nAnalog Neural Networks of Limited Precision I: \\nComputing with Multilinear Threshold Functions \\n(Preliminary Version) \\nZoran Obradovic and Ian Parberry \\nDepartment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topic  Contribution %  Paper Num  \\\n",
       "Topic1              T1         0.99930        942   \n",
       "Topic2              T2         0.00033        108   \n",
       "Topic3              T3         0.76997        149   \n",
       "Topic4              T4         0.99945       1103   \n",
       "Topic5              T5         0.99949        213   \n",
       "Topic6              T6         0.98993       1084   \n",
       "Topic7              T7         0.99956        266   \n",
       "Topic8              T8         0.97427        889   \n",
       "Topic9              T9         0.99929        281   \n",
       "Topic10            T10         0.00033        108   \n",
       "Topic11            T11         0.99917       1289   \n",
       "Topic12            T12         0.00033        108   \n",
       "Topic13            T13         0.00033        108   \n",
       "Topic14            T14         0.00033        108   \n",
       "Topic15            T15         0.99940       1691   \n",
       "Topic16            T16         0.00033        108   \n",
       "Topic17            T17         0.00033        108   \n",
       "Topic18            T18         0.99942        348   \n",
       "Topic19            T19         0.00033        108   \n",
       "Topic20            T20         0.68094        199   \n",
       "\n",
       "                                                                                                                                                                                                           Topic  \\\n",
       "Topic1                                       neuron, circuit, chip, analog, current, signal, voltage, channel, vlsi, implementation, bit, noise, pulse, processor, design, synapse, parallel, fig, line, digital   \n",
       "Topic2                                        image, feature, structure, state, layer, neuron, distribution, local, cell, motion, recognition, node, net, matrix, object, gaussian, sequence, line, size, hidden   \n",
       "Topic3       motor, frequency, auditory, sound, template, command, spectrum, acoustic, syllable, control, feedback, amplitude, motor command, song, production, representation, onset, harmonic, temporal, phase   \n",
       "Topic4                                          cell, neuron, response, visual, stimulus, activity, spike, field, synaptic, motion, direction, firing, cortex, signal, orientation, spatial, eye, rate, map, fig   \n",
       "Topic5                      image, feature, recognition, layer, hidden, task, object, speech, representation, trained, test, classification, net, classifier, level, architecture, class, experiment, rule, node   \n",
       "Topic6                                state, dynamic, matrix, equation, rule, recurrent, gradient, fixed, neuron, solution, node, signal, hidden, sequence, net, activation, attractor, source, step, connection   \n",
       "Topic7                                       sequence, chain, region, structure, protein, prediction, hmms, site, receptor, gene, class, human, positive, distance, length, mouse, negative, cell, domain, sheet   \n",
       "Topic8                    memory, word, context, similarity, item, recall, phoneme, activation, probability, representation, short, list, association, address, short term, state, serial, store, storage, phone   \n",
       "Topic9                                   ii, activation, winner, region, take, winner take, texture, ii ii, behavior, self, saliency, competitive, wta, connection, binding, iii, edge, feedback, search, sensor   \n",
       "Topic10                              state, cell, distribution, neuron, probability, control, response, signal, task, layer, rate, architecture, random, hidden, test, image, fig, change, field, generalization   \n",
       "Topic11  generalization, face, hidden unit, hidden, capacity, pca, image, teacher, student, principal, generalization error, committee, component, principal component, correlation, limit, facial, training ...   \n",
       "Topic12                                        feature, image, distribution, neuron, state, class, hidden, probability, layer, node, equation, size, rate, matrix, line, prediction, signal, et, noise, variable   \n",
       "Topic13                                              image, state, cell, object, rule, layer, et al, neuron, distribution, step, visual, signal, field, dynamic, feature, probability, map, et, solution, matrix   \n",
       "Topic14                                neuron, map, state, cell, rate, hidden, probability, field, equation, representation, node, signal, dynamic, layer, et al, sequence, test, prediction, recognition, noise   \n",
       "Topic15              distribution, probability, variable, gaussian, class, approximation, estimate, sample, density, noise, mixture, matrix, log, likelihood, prior, optimal, xi, variance, bayesian, prediction   \n",
       "Topic16                                       state, neuron, rule, probability, layer, rate, memory, image, distribution, equation, signal, solution, response, class, theory, et, step, feature, variable, high   \n",
       "Topic17                            state, feature, probability, image, layer, cell, field, neuron, task, rate, recognition, dynamic, variable, distribution, rule, representation, control, equation, net, class   \n",
       "Topic18       state, control, action, policy, reinforcement, optimal, task, step, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm   \n",
       "Topic19                               state, control, feature, probability, neuron, hidden, architecture, task, rate, level, estimate, local, et, distribution, component, net, response, signal, image, optimal   \n",
       "Topic20                                            threshold, theorem, bound, net, proof, size, polynomial, layer, let, depth, gate, neural net, bit, bounded, binary, constant, circuit, every, assume, integer   \n",
       "\n",
       "                                                                                                                                                                                                      Paper Name  \n",
       "Topic1   Single Transistor Learning Synapses \\nPaul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\n(SlS) 95- 2S12 \\npaul@hobiecat.pcmp.calt...  \n",
       "Topic2   794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic3   795 \\nSONG LEARNING IN BIRDS \\nM. Konishi \\nDivision of Biology \\nCalifornia Institute of Technology \\nABSTRACT\\nBirds sing to communicate. Male birds use song to advertise their territories and \\...  \n",
       "Topic4   Simulation of a Thalamocortical Circuit for \\nComputing Directional Heading in the Rat \\nHugh T. Blair* \\nDepartment of Psychology \\nYale University \\nNew Haven, CT 06520-8205 \\ntadb @minerva. cis...  \n",
       "Topic5   266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...  \n",
       "Topic6   Harmony Networks Do Not Work \\nRen5 Gourley \\nSchool of Computing Science \\nSimon Fraser University \\nBurnaby, B.C., V5A 1S6, Canada \\ngourley@mprgate.mpr.ca \\nAbstract \\nHarmony networks have be...  \n",
       "Topic7   A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...  \n",
       "Topic8   A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...  \n",
       "Topic9   44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...  \n",
       "Topic10  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic11  I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...  \n",
       "Topic12  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic13  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic14  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic15  Optimal Kernel Shapes for Local Linear \\nRegression \\nDirk Ormoneit Trevor Hastie \\nDepartment of Statistics \\nStanford University \\nStanford, CA 94305-4065 \\normoneit@stat. stanford. edu \\nAbstra...  \n",
       "Topic16  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic17  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic18  Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...  \n",
       "Topic19  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic20  702 Obradovic and Parberry \\nAnalog Neural Networks of Limited Precision I: \\nComputing with Multilinear Threshold Functions \\n(Preliminary Version) \\nZoran Obradovic and Ian Parberry \\nDepartment...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_contrib_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_contrib_topics.index\n",
    "contrib_perc = max_contrib_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0] for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can see that some topics have a very\n",
    "poor representation of almost 0% in the corpus and so we see the same paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics with a good\n",
    "contribution (almost 100% dominance) showcase papers that are closely correlated\n",
    "with the theme conveyed by the corresponding topic, including reinforcement learning,\n",
    "Bayesian and Gaussian mixture models, neural models on VLSI, and transistors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
