{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd11f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "import utils as ut\n",
    "import nltk\n",
    "import textblob\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b103db",
   "metadata": {},
   "source": [
    "## load our IMDB review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c467474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie_reviews.csv.bz2', compression='bz2')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470bf735",
   "metadata": {},
   "source": [
    "##  TextBlob lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f960f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -0.3625\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 0.16666666666666674\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -0.037239583333333326\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3bf9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110dd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3119f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7669\n",
      "Precision: 0.767\n",
      "Recall: 0.7669\n",
      "F1 Score: 0.7668\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.78      0.77      7510\n",
      "    negative       0.77      0.76      0.76      7490\n",
      "\n",
      "    accuracy                           0.77     15000\n",
      "   macro avg       0.77      0.77      0.77     15000\n",
      "weighted avg       0.77      0.77      0.77     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  codes=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                codes=level_labels)) \n",
    "    print(cm_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab88f3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5835     1675\n",
      "        negative       1822     5668\n"
     ]
    }
   ],
   "source": [
    "display_confusion_matrix(true_labels=test_sentiments, predicted_labels=predicted_sentiments,classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032941d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
