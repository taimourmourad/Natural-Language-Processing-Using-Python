{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-873dce4dc78b>:72: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array([sent2features(s) for s in sentences])\n",
      "<ipython-input-30-873dce4dc78b>:73: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array([sent2labels(s) for s in sentences])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ner_dataset.csv.gz', compression='gzip', encoding='ISO-8859-1')\n",
    "\n",
    "df =df.fillna(method='ffill')#inplace NaN instead of 0\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:#len()means here length of all characters and spaces in sent.\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]#len=11 so range 0, 10\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "# just a function\n",
    "agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(), \n",
    "                                                   s['POS'].values.tolist(), \n",
    "                                                   s['Tag'].values.tolist())]#s=sentence\n",
    "\n",
    "grouped_df = df.groupby('Sentence #').apply(agg_func)\n",
    "\n",
    "print(grouped_df[grouped_df.index == 'Sentence: 1'].values)\n",
    "\n",
    "sentences = [s for s in grouped_df]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([sent2features(s) for s in sentences])\n",
    "y = np.array([sent2labels(s) for s in sentences])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',\n",
    "                           c1=0.1,\n",
    "                           c2=0.1,\n",
    "                           max_iterations=50,\n",
    "                           all_possible_transitions=True,\n",
    "                           verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"Three more countries have joined an \"international grand\n",
    "committee\" of parliaments, adding\n",
    "\n",
    "to calls forFacebook’s boss, Mark\n",
    "Zuckerberg, to give evidence on misinformation to the coalition. Brazil,\n",
    "Latvia and Singapore bring the total to eight different parliaments across\n",
    "the world, with plans to send representatives to London on 27 November\n",
    "with the intention of hearing from Zuckerberg. Since the Cambridge\n",
    "Analytica scandal broke, the Facebook chief has only appeared in front of\n",
    "two legislatures: the American Senate and House of Representatives, and\n",
    "the European parliament. Facebook has consistently rebuffed attempts from\n",
    "others, including the UK and Canadian parliaments, to hear from Zuckerberg.\n",
    "He added that an article in the New York Times on Thursday, in which the\n",
    "paper alleged a pattern of behaviour from Facebook to \"delay, deny and\n",
    "deflect\" negative news stories, \"raises further questions about how recent\n",
    "data breaches were allegedly dealt with within Facebook.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Three', 'CD'),\n",
       " ('more', 'JJR'),\n",
       " ('countries', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('joined', 'VBN'),\n",
       " ('an', 'DT'),\n",
       " ('``', '``'),\n",
       " ('international', 'JJ'),\n",
       " ('grandcommittee', 'NN'),\n",
       " (\"''\", \"''\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text_tokens = nltk.word_tokenize(text)\n",
    "text_pos = nltk.pos_tag(text_tokens)\n",
    "text_pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = crf.predict(features)\n",
    "doc_labels = labels[0]\n",
    "doc_labels[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ner = [(token, tag) for token, tag in zip(text_tokens, doc_labels)]\n",
    "print(text_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and display all named entities\n",
    "named_entities = []\n",
    "temp_entity_name = \"\n",
    "temp_named_entity = None\n",
    "for term, tag in text_ner:\n",
    "    if tag != 'O':\n",
    "        temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "        temp_named_entity = (temp_entity_name, tag)\n",
    "    else:\n",
    "        if temp_named_entity:\n",
    "            named_entities.append(temp_named_entity)\n",
    "            temp_entity_name = \"\n",
    "            temp_named_entity = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(named_entities, columns=['Entity', 'Tag'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
